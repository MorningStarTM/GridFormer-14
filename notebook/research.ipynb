{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\github_clone\\\\GridFormer-14'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridformer.vq_vae import VQVAE\n",
    "from gridformer.trainer import VQTrainer\n",
    "from gridformer.Utils.utils import Config\n",
    "from gridformer.Utils.dataset import load_observation, ObsDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import lightsim2grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_yaml('config.yml')\n",
    "model = VQVAE(config=config)\n",
    "trainer = VQTrainer(config=config, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = load_observation(\"C:\\\\Users\\\\Ernest\\\\Downloads\\\\topo_data\", start=0, end=5)\n",
    "dataset = ObsDataset(obs, device=model.device)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "data_variance = np.var(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Reconstruction Loss: 0.1079, \n",
      "Model saved at 0.108\n",
      "Epoch 2 - Reconstruction Loss: 0.1027, \n",
      "Model saved at 0.103\n",
      "Epoch 3 - Reconstruction Loss: 0.0866, \n",
      "Model saved at 0.087\n",
      "Epoch 4 - Reconstruction Loss: 0.0879, \n",
      "Epoch 5 - Reconstruction Loss: 0.1068, \n",
      "Epoch 6 - Reconstruction Loss: 0.0788, \n",
      "Model saved at 0.079\n",
      "Epoch 7 - Reconstruction Loss: 0.1074, \n",
      "Epoch 8 - Reconstruction Loss: 0.0928, \n",
      "Epoch 9 - Reconstruction Loss: 0.0744, \n",
      "Model saved at 0.074\n",
      "Epoch 10 - Reconstruction Loss: 0.1070, \n",
      "Epoch 11 - Reconstruction Loss: 0.0943, \n",
      "Epoch 12 - Reconstruction Loss: 0.0935, \n",
      "Epoch 13 - Reconstruction Loss: 0.0900, \n",
      "Epoch 14 - Reconstruction Loss: 0.0988, \n",
      "Epoch 15 - Reconstruction Loss: 0.0988, \n",
      "Epoch 16 - Reconstruction Loss: 0.0978, \n",
      "Epoch 17 - Reconstruction Loss: 0.0861, \n",
      "Epoch 18 - Reconstruction Loss: 0.1068, \n",
      "Epoch 19 - Reconstruction Loss: 0.1105, \n",
      "Epoch 20 - Reconstruction Loss: 0.0960, \n",
      "Epoch 21 - Reconstruction Loss: 0.0795, \n",
      "Epoch 22 - Reconstruction Loss: 0.1002, \n",
      "Epoch 23 - Reconstruction Loss: 0.1086, \n",
      "Epoch 24 - Reconstruction Loss: 0.1021, \n",
      "Epoch 25 - Reconstruction Loss: 0.1153, \n",
      "Epoch 26 - Reconstruction Loss: 0.1058, \n",
      "Epoch 27 - Reconstruction Loss: 0.0892, \n",
      "Epoch 28 - Reconstruction Loss: 0.0918, \n",
      "Epoch 29 - Reconstruction Loss: 0.0875, \n",
      "Epoch 30 - Reconstruction Loss: 0.0946, \n",
      "Epoch 31 - Reconstruction Loss: 0.0992, \n",
      "Epoch 32 - Reconstruction Loss: 0.1088, \n",
      "Epoch 33 - Reconstruction Loss: 0.0959, \n",
      "Epoch 34 - Reconstruction Loss: 0.0871, \n",
      "Epoch 35 - Reconstruction Loss: 0.0772, \n",
      "Epoch 36 - Reconstruction Loss: 0.0872, \n",
      "Epoch 37 - Reconstruction Loss: 0.1059, \n",
      "Epoch 38 - Reconstruction Loss: 0.1009, \n",
      "Epoch 39 - Reconstruction Loss: 0.0891, \n",
      "Epoch 40 - Reconstruction Loss: 0.1050, \n",
      "Epoch 41 - Reconstruction Loss: 0.1122, \n",
      "Epoch 42 - Reconstruction Loss: 0.0969, \n",
      "Epoch 43 - Reconstruction Loss: 0.1004, \n",
      "Epoch 44 - Reconstruction Loss: 0.0942, \n",
      "Epoch 45 - Reconstruction Loss: 0.1019, \n",
      "Epoch 46 - Reconstruction Loss: 0.0885, \n",
      "Epoch 47 - Reconstruction Loss: 0.0814, \n",
      "Epoch 48 - Reconstruction Loss: 0.1018, \n",
      "Epoch 49 - Reconstruction Loss: 0.1002, \n",
      "Epoch 50 - Reconstruction Loss: 0.0996, \n",
      "Epoch 51 - Reconstruction Loss: 0.0922, \n",
      "Epoch 52 - Reconstruction Loss: 0.0966, \n",
      "Epoch 53 - Reconstruction Loss: 0.1131, \n",
      "Epoch 54 - Reconstruction Loss: 0.0888, \n",
      "Epoch 55 - Reconstruction Loss: 0.0897, \n",
      "Epoch 56 - Reconstruction Loss: 0.0988, \n",
      "Epoch 57 - Reconstruction Loss: 0.0889, \n",
      "Epoch 58 - Reconstruction Loss: 0.1093, \n",
      "Epoch 59 - Reconstruction Loss: 0.1041, \n",
      "Epoch 60 - Reconstruction Loss: 0.1193, \n",
      "Epoch 61 - Reconstruction Loss: 0.0959, \n",
      "Epoch 62 - Reconstruction Loss: 0.0852, \n",
      "Epoch 63 - Reconstruction Loss: 0.0884, \n",
      "Epoch 64 - Reconstruction Loss: 0.0983, \n",
      "Epoch 65 - Reconstruction Loss: 0.0784, \n",
      "Epoch 66 - Reconstruction Loss: 0.0904, \n",
      "Epoch 67 - Reconstruction Loss: 0.1140, \n",
      "Epoch 68 - Reconstruction Loss: 0.1005, \n",
      "Epoch 69 - Reconstruction Loss: 0.0889, \n",
      "Epoch 70 - Reconstruction Loss: 0.1012, \n",
      "Epoch 71 - Reconstruction Loss: 0.0968, \n",
      "Epoch 72 - Reconstruction Loss: 0.0724, \n",
      "Model saved at 0.072\n",
      "Epoch 73 - Reconstruction Loss: 0.0763, \n",
      "Epoch 74 - Reconstruction Loss: 0.1031, \n",
      "Epoch 75 - Reconstruction Loss: 0.0922, \n",
      "Epoch 76 - Reconstruction Loss: 0.0995, \n",
      "Epoch 77 - Reconstruction Loss: 0.0967, \n",
      "Epoch 78 - Reconstruction Loss: 0.1078, \n",
      "Epoch 79 - Reconstruction Loss: 0.0807, \n",
      "Epoch 80 - Reconstruction Loss: 0.0869, \n",
      "Epoch 81 - Reconstruction Loss: 0.0959, \n",
      "Epoch 82 - Reconstruction Loss: 0.0870, \n",
      "Epoch 83 - Reconstruction Loss: 0.0859, \n",
      "Epoch 84 - Reconstruction Loss: 0.1081, \n",
      "Epoch 85 - Reconstruction Loss: 0.0704, \n",
      "Model saved at 0.070\n",
      "Epoch 86 - Reconstruction Loss: 0.1039, \n",
      "Epoch 87 - Reconstruction Loss: 0.0883, \n",
      "Epoch 88 - Reconstruction Loss: 0.1076, \n",
      "Epoch 89 - Reconstruction Loss: 0.1008, \n",
      "Epoch 90 - Reconstruction Loss: 0.0902, \n",
      "Epoch 91 - Reconstruction Loss: 0.0826, \n",
      "Epoch 92 - Reconstruction Loss: 0.1026, \n",
      "Epoch 93 - Reconstruction Loss: 0.0794, \n",
      "Epoch 94 - Reconstruction Loss: 0.1113, \n",
      "Epoch 95 - Reconstruction Loss: 0.1136, \n",
      "Epoch 96 - Reconstruction Loss: 0.0759, \n",
      "Epoch 97 - Reconstruction Loss: 0.0986, \n",
      "Epoch 98 - Reconstruction Loss: 0.1024, \n",
      "Epoch 99 - Reconstruction Loss: 0.1043, \n",
      "Epoch 100 - Reconstruction Loss: 0.0849, \n",
      "Epoch 101 - Reconstruction Loss: 0.0910, \n",
      "Epoch 102 - Reconstruction Loss: 0.1030, \n",
      "Epoch 103 - Reconstruction Loss: 0.1089, \n",
      "Epoch 104 - Reconstruction Loss: 0.0837, \n",
      "Epoch 105 - Reconstruction Loss: 0.0947, \n",
      "Epoch 106 - Reconstruction Loss: 0.1149, \n",
      "Epoch 107 - Reconstruction Loss: 0.0952, \n",
      "Epoch 108 - Reconstruction Loss: 0.0889, \n",
      "Epoch 109 - Reconstruction Loss: 0.0869, \n",
      "Epoch 110 - Reconstruction Loss: 0.1058, \n",
      "Epoch 111 - Reconstruction Loss: 0.0948, \n",
      "Epoch 112 - Reconstruction Loss: 0.0825, \n",
      "Epoch 113 - Reconstruction Loss: 0.0833, \n",
      "Epoch 114 - Reconstruction Loss: 0.0813, \n",
      "Epoch 115 - Reconstruction Loss: 0.0964, \n",
      "Epoch 116 - Reconstruction Loss: 0.1121, \n",
      "Epoch 117 - Reconstruction Loss: 0.0884, \n",
      "Epoch 118 - Reconstruction Loss: 0.0836, \n",
      "Epoch 119 - Reconstruction Loss: 0.0989, \n",
      "Epoch 120 - Reconstruction Loss: 0.0698, \n",
      "Model saved at 0.070\n",
      "Epoch 121 - Reconstruction Loss: 0.0864, \n",
      "Epoch 122 - Reconstruction Loss: 0.0921, \n",
      "Epoch 123 - Reconstruction Loss: 0.0956, \n",
      "Epoch 124 - Reconstruction Loss: 0.0916, \n",
      "Epoch 125 - Reconstruction Loss: 0.0935, \n",
      "Epoch 126 - Reconstruction Loss: 0.0979, \n",
      "Epoch 127 - Reconstruction Loss: 0.1007, \n",
      "Epoch 128 - Reconstruction Loss: 0.0932, \n",
      "Epoch 129 - Reconstruction Loss: 0.0956, \n",
      "Epoch 130 - Reconstruction Loss: 0.0866, \n",
      "Epoch 131 - Reconstruction Loss: 0.0906, \n",
      "Epoch 132 - Reconstruction Loss: 0.0945, \n",
      "Epoch 133 - Reconstruction Loss: 0.0930, \n",
      "Epoch 134 - Reconstruction Loss: 0.0867, \n",
      "Epoch 135 - Reconstruction Loss: 0.0993, \n",
      "Epoch 136 - Reconstruction Loss: 0.0965, \n",
      "Epoch 137 - Reconstruction Loss: 0.1180, \n",
      "Epoch 138 - Reconstruction Loss: 0.0994, \n",
      "Epoch 139 - Reconstruction Loss: 0.1004, \n",
      "Epoch 140 - Reconstruction Loss: 0.0907, \n",
      "Epoch 141 - Reconstruction Loss: 0.1045, \n",
      "Epoch 142 - Reconstruction Loss: 0.1012, \n",
      "Epoch 143 - Reconstruction Loss: 0.0965, \n",
      "Epoch 144 - Reconstruction Loss: 0.1024, \n",
      "Epoch 145 - Reconstruction Loss: 0.1085, \n",
      "Epoch 146 - Reconstruction Loss: 0.0904, \n",
      "Epoch 147 - Reconstruction Loss: 0.0845, \n",
      "Epoch 148 - Reconstruction Loss: 0.0930, \n",
      "Epoch 149 - Reconstruction Loss: 0.1225, \n",
      "Epoch 150 - Reconstruction Loss: 0.0988, \n",
      "Epoch 151 - Reconstruction Loss: 0.1107, \n",
      "Epoch 152 - Reconstruction Loss: 0.0971, \n",
      "Epoch 153 - Reconstruction Loss: 0.0930, \n",
      "Epoch 154 - Reconstruction Loss: 0.1008, \n",
      "Epoch 155 - Reconstruction Loss: 0.0807, \n",
      "Epoch 156 - Reconstruction Loss: 0.0858, \n",
      "Epoch 157 - Reconstruction Loss: 0.0963, \n",
      "Epoch 158 - Reconstruction Loss: 0.1018, \n",
      "Epoch 159 - Reconstruction Loss: 0.1005, \n",
      "Epoch 160 - Reconstruction Loss: 0.0896, \n",
      "Epoch 161 - Reconstruction Loss: 0.1074, \n",
      "Epoch 162 - Reconstruction Loss: 0.1059, \n",
      "Epoch 163 - Reconstruction Loss: 0.0905, \n",
      "Epoch 164 - Reconstruction Loss: 0.0980, \n",
      "Epoch 165 - Reconstruction Loss: 0.1029, \n",
      "Epoch 166 - Reconstruction Loss: 0.1037, \n",
      "Epoch 167 - Reconstruction Loss: 0.0851, \n",
      "Epoch 168 - Reconstruction Loss: 0.1063, \n",
      "Epoch 169 - Reconstruction Loss: 0.0780, \n",
      "Epoch 170 - Reconstruction Loss: 0.0817, \n",
      "Epoch 171 - Reconstruction Loss: 0.1059, \n",
      "Epoch 172 - Reconstruction Loss: 0.1018, \n",
      "Epoch 173 - Reconstruction Loss: 0.0917, \n",
      "Epoch 174 - Reconstruction Loss: 0.0986, \n",
      "Epoch 175 - Reconstruction Loss: 0.0938, \n",
      "Epoch 176 - Reconstruction Loss: 0.0932, \n",
      "Epoch 177 - Reconstruction Loss: 0.1156, \n",
      "Epoch 178 - Reconstruction Loss: 0.1065, \n",
      "Epoch 179 - Reconstruction Loss: 0.0879, \n",
      "Epoch 180 - Reconstruction Loss: 0.1148, \n",
      "Epoch 181 - Reconstruction Loss: 0.0752, \n",
      "Epoch 182 - Reconstruction Loss: 0.0980, \n",
      "Epoch 183 - Reconstruction Loss: 0.0919, \n",
      "Epoch 184 - Reconstruction Loss: 0.0995, \n",
      "Epoch 185 - Reconstruction Loss: 0.0939, \n",
      "Epoch 186 - Reconstruction Loss: 0.0915, \n",
      "Epoch 187 - Reconstruction Loss: 0.0906, \n",
      "Epoch 188 - Reconstruction Loss: 0.1045, \n",
      "Epoch 189 - Reconstruction Loss: 0.0955, \n",
      "Epoch 190 - Reconstruction Loss: 0.1083, \n",
      "Epoch 191 - Reconstruction Loss: 0.0945, \n",
      "Epoch 192 - Reconstruction Loss: 0.1058, \n",
      "Epoch 193 - Reconstruction Loss: 0.0959, \n",
      "Epoch 194 - Reconstruction Loss: 0.0858, \n",
      "Epoch 195 - Reconstruction Loss: 0.0836, \n",
      "Epoch 196 - Reconstruction Loss: 0.0914, \n",
      "Epoch 197 - Reconstruction Loss: 0.0985, \n",
      "Epoch 198 - Reconstruction Loss: 0.1081, \n",
      "Epoch 199 - Reconstruction Loss: 0.0920, \n",
      "Epoch 200 - Reconstruction Loss: 0.1006, \n",
      "Epoch 201 - Reconstruction Loss: 0.0979, \n",
      "Epoch 202 - Reconstruction Loss: 0.0746, \n",
      "Epoch 203 - Reconstruction Loss: 0.0959, \n",
      "Epoch 204 - Reconstruction Loss: 0.1009, \n",
      "Epoch 205 - Reconstruction Loss: 0.0961, \n",
      "Epoch 206 - Reconstruction Loss: 0.1125, \n",
      "Epoch 207 - Reconstruction Loss: 0.0943, \n",
      "Epoch 208 - Reconstruction Loss: 0.0895, \n",
      "Epoch 209 - Reconstruction Loss: 0.0963, \n",
      "Epoch 210 - Reconstruction Loss: 0.0975, \n",
      "Epoch 211 - Reconstruction Loss: 0.0793, \n",
      "Epoch 212 - Reconstruction Loss: 0.0926, \n",
      "Epoch 213 - Reconstruction Loss: 0.1048, \n",
      "Epoch 214 - Reconstruction Loss: 0.1011, \n",
      "Epoch 215 - Reconstruction Loss: 0.1112, \n",
      "Epoch 216 - Reconstruction Loss: 0.0960, \n",
      "Epoch 217 - Reconstruction Loss: 0.0921, \n",
      "Epoch 218 - Reconstruction Loss: 0.0920, \n",
      "Epoch 219 - Reconstruction Loss: 0.0845, \n",
      "Epoch 220 - Reconstruction Loss: 0.0965, \n",
      "Epoch 221 - Reconstruction Loss: 0.1134, \n",
      "Epoch 222 - Reconstruction Loss: 0.0829, \n",
      "Epoch 223 - Reconstruction Loss: 0.0864, \n",
      "Epoch 224 - Reconstruction Loss: 0.1032, \n",
      "Epoch 225 - Reconstruction Loss: 0.0888, \n",
      "Epoch 226 - Reconstruction Loss: 0.1063, \n",
      "Epoch 227 - Reconstruction Loss: 0.0848, \n",
      "Epoch 228 - Reconstruction Loss: 0.1008, \n",
      "Epoch 229 - Reconstruction Loss: 0.0932, \n",
      "Epoch 230 - Reconstruction Loss: 0.0786, \n",
      "Epoch 231 - Reconstruction Loss: 0.0945, \n",
      "Epoch 232 - Reconstruction Loss: 0.1110, \n",
      "Epoch 233 - Reconstruction Loss: 0.0878, \n",
      "Epoch 234 - Reconstruction Loss: 0.1005, \n",
      "Epoch 235 - Reconstruction Loss: 0.1161, \n",
      "Epoch 236 - Reconstruction Loss: 0.1018, \n",
      "Epoch 237 - Reconstruction Loss: 0.1127, \n",
      "Epoch 238 - Reconstruction Loss: 0.1021, \n",
      "Epoch 239 - Reconstruction Loss: 0.0974, \n",
      "Epoch 240 - Reconstruction Loss: 0.1038, \n",
      "Epoch 241 - Reconstruction Loss: 0.0939, \n",
      "Epoch 242 - Reconstruction Loss: 0.0933, \n",
      "Epoch 243 - Reconstruction Loss: 0.0980, \n",
      "Epoch 244 - Reconstruction Loss: 0.1129, \n",
      "Epoch 245 - Reconstruction Loss: 0.0950, \n",
      "Epoch 246 - Reconstruction Loss: 0.0978, \n",
      "Epoch 247 - Reconstruction Loss: 0.1043, \n",
      "Epoch 248 - Reconstruction Loss: 0.0866, \n",
      "Epoch 249 - Reconstruction Loss: 0.1047, \n",
      "Epoch 250 - Reconstruction Loss: 0.0736, \n",
      "Epoch 251 - Reconstruction Loss: 0.1099, \n",
      "Epoch 252 - Reconstruction Loss: 0.0938, \n",
      "Epoch 253 - Reconstruction Loss: 0.0971, \n",
      "Epoch 254 - Reconstruction Loss: 0.0941, \n",
      "Epoch 255 - Reconstruction Loss: 0.0907, \n",
      "Epoch 256 - Reconstruction Loss: 0.0952, \n",
      "Epoch 257 - Reconstruction Loss: 0.0907, \n",
      "Epoch 258 - Reconstruction Loss: 0.0962, \n",
      "Epoch 259 - Reconstruction Loss: 0.1048, \n",
      "Epoch 260 - Reconstruction Loss: 0.0899, \n",
      "Epoch 261 - Reconstruction Loss: 0.1008, \n",
      "Epoch 262 - Reconstruction Loss: 0.0831, \n",
      "Epoch 263 - Reconstruction Loss: 0.0684, \n",
      "Model saved at 0.068\n",
      "Epoch 264 - Reconstruction Loss: 0.0825, \n",
      "Epoch 265 - Reconstruction Loss: 0.0757, \n",
      "Epoch 266 - Reconstruction Loss: 0.0913, \n",
      "Epoch 267 - Reconstruction Loss: 0.0887, \n",
      "Epoch 268 - Reconstruction Loss: 0.1072, \n",
      "Epoch 269 - Reconstruction Loss: 0.0865, \n",
      "Epoch 270 - Reconstruction Loss: 0.0912, \n",
      "Epoch 271 - Reconstruction Loss: 0.0821, \n",
      "Epoch 272 - Reconstruction Loss: 0.0903, \n",
      "Epoch 273 - Reconstruction Loss: 0.0666, \n",
      "Model saved at 0.067\n",
      "Epoch 274 - Reconstruction Loss: 0.1021, \n",
      "Epoch 275 - Reconstruction Loss: 0.0816, \n",
      "Epoch 276 - Reconstruction Loss: 0.0949, \n",
      "Epoch 277 - Reconstruction Loss: 0.1150, \n",
      "Epoch 278 - Reconstruction Loss: 0.0836, \n",
      "Epoch 279 - Reconstruction Loss: 0.1023, \n",
      "Epoch 280 - Reconstruction Loss: 0.0793, \n",
      "Epoch 281 - Reconstruction Loss: 0.0847, \n",
      "Epoch 282 - Reconstruction Loss: 0.0900, \n",
      "Epoch 283 - Reconstruction Loss: 0.0943, \n",
      "Epoch 284 - Reconstruction Loss: 0.1014, \n",
      "Epoch 285 - Reconstruction Loss: 0.1072, \n",
      "Epoch 286 - Reconstruction Loss: 0.0855, \n",
      "Epoch 287 - Reconstruction Loss: 0.1009, \n",
      "Epoch 288 - Reconstruction Loss: 0.1039, \n",
      "Epoch 289 - Reconstruction Loss: 0.0727, \n",
      "Epoch 290 - Reconstruction Loss: 0.0991, \n",
      "Epoch 291 - Reconstruction Loss: 0.1054, \n",
      "Epoch 292 - Reconstruction Loss: 0.1064, \n",
      "Epoch 293 - Reconstruction Loss: 0.0925, \n",
      "Epoch 294 - Reconstruction Loss: 0.1037, \n",
      "Epoch 295 - Reconstruction Loss: 0.1011, \n",
      "Epoch 296 - Reconstruction Loss: 0.0857, \n",
      "Epoch 297 - Reconstruction Loss: 0.0793, \n",
      "Epoch 298 - Reconstruction Loss: 0.0866, \n",
      "Epoch 299 - Reconstruction Loss: 0.0874, \n",
      "Epoch 300 - Reconstruction Loss: 0.0994, \n",
      "Epoch 301 - Reconstruction Loss: 0.0839, \n",
      "Epoch 302 - Reconstruction Loss: 0.0961, \n",
      "Epoch 303 - Reconstruction Loss: 0.1022, \n",
      "Epoch 304 - Reconstruction Loss: 0.0802, \n",
      "Epoch 305 - Reconstruction Loss: 0.1010, \n",
      "Epoch 306 - Reconstruction Loss: 0.0904, \n",
      "Epoch 307 - Reconstruction Loss: 0.1007, \n",
      "Epoch 308 - Reconstruction Loss: 0.0923, \n",
      "Epoch 309 - Reconstruction Loss: 0.0881, \n",
      "Epoch 310 - Reconstruction Loss: 0.0965, \n",
      "Epoch 311 - Reconstruction Loss: 0.0989, \n",
      "Epoch 312 - Reconstruction Loss: 0.1065, \n",
      "Epoch 313 - Reconstruction Loss: 0.0947, \n",
      "Epoch 314 - Reconstruction Loss: 0.1029, \n",
      "Epoch 315 - Reconstruction Loss: 0.1039, \n",
      "Epoch 316 - Reconstruction Loss: 0.1118, \n",
      "Epoch 317 - Reconstruction Loss: 0.0975, \n",
      "Epoch 318 - Reconstruction Loss: 0.0957, \n",
      "Epoch 319 - Reconstruction Loss: 0.0861, \n",
      "Epoch 320 - Reconstruction Loss: 0.0922, \n",
      "Epoch 321 - Reconstruction Loss: 0.1088, \n",
      "Epoch 322 - Reconstruction Loss: 0.0855, \n",
      "Epoch 323 - Reconstruction Loss: 0.1047, \n",
      "Epoch 324 - Reconstruction Loss: 0.0958, \n",
      "Epoch 325 - Reconstruction Loss: 0.0924, \n",
      "Epoch 326 - Reconstruction Loss: 0.0879, \n",
      "Epoch 327 - Reconstruction Loss: 0.0939, \n",
      "Epoch 328 - Reconstruction Loss: 0.1041, \n",
      "Epoch 329 - Reconstruction Loss: 0.0778, \n",
      "Epoch 330 - Reconstruction Loss: 0.0839, \n",
      "Epoch 331 - Reconstruction Loss: 0.0825, \n",
      "Epoch 332 - Reconstruction Loss: 0.1041, \n",
      "Epoch 333 - Reconstruction Loss: 0.1063, \n",
      "Epoch 334 - Reconstruction Loss: 0.0994, \n",
      "Epoch 335 - Reconstruction Loss: 0.0911, \n",
      "Epoch 336 - Reconstruction Loss: 0.0737, \n",
      "Epoch 337 - Reconstruction Loss: 0.1077, \n",
      "Epoch 338 - Reconstruction Loss: 0.1026, \n",
      "Epoch 339 - Reconstruction Loss: 0.1045, \n",
      "Epoch 340 - Reconstruction Loss: 0.1048, \n",
      "Epoch 341 - Reconstruction Loss: 0.0966, \n",
      "Epoch 342 - Reconstruction Loss: 0.1041, \n",
      "Epoch 343 - Reconstruction Loss: 0.1053, \n",
      "Epoch 344 - Reconstruction Loss: 0.0917, \n",
      "Epoch 345 - Reconstruction Loss: 0.1117, \n",
      "Epoch 346 - Reconstruction Loss: 0.1224, \n",
      "Epoch 347 - Reconstruction Loss: 0.1169, \n",
      "Epoch 348 - Reconstruction Loss: 0.0910, \n",
      "Epoch 349 - Reconstruction Loss: 0.0837, \n",
      "Epoch 350 - Reconstruction Loss: 0.0796, \n",
      "Epoch 351 - Reconstruction Loss: 0.0982, \n",
      "Epoch 352 - Reconstruction Loss: 0.0965, \n",
      "Epoch 353 - Reconstruction Loss: 0.0799, \n",
      "Epoch 354 - Reconstruction Loss: 0.1136, \n",
      "Epoch 355 - Reconstruction Loss: 0.0987, \n",
      "Epoch 356 - Reconstruction Loss: 0.0928, \n",
      "Epoch 357 - Reconstruction Loss: 0.0961, \n",
      "Epoch 358 - Reconstruction Loss: 0.0873, \n",
      "Epoch 359 - Reconstruction Loss: 0.1050, \n",
      "Epoch 360 - Reconstruction Loss: 0.1064, \n",
      "Epoch 361 - Reconstruction Loss: 0.0872, \n",
      "Epoch 362 - Reconstruction Loss: 0.0795, \n",
      "Epoch 363 - Reconstruction Loss: 0.0825, \n",
      "Epoch 364 - Reconstruction Loss: 0.1019, \n",
      "Epoch 365 - Reconstruction Loss: 0.0929, \n",
      "Epoch 366 - Reconstruction Loss: 0.0740, \n",
      "Epoch 367 - Reconstruction Loss: 0.0977, \n",
      "Epoch 368 - Reconstruction Loss: 0.0932, \n",
      "Epoch 369 - Reconstruction Loss: 0.0667, \n",
      "Epoch 370 - Reconstruction Loss: 0.1056, \n",
      "Epoch 371 - Reconstruction Loss: 0.0939, \n",
      "Epoch 372 - Reconstruction Loss: 0.1065, \n",
      "Epoch 373 - Reconstruction Loss: 0.0878, \n",
      "Epoch 374 - Reconstruction Loss: 0.1070, \n",
      "Epoch 375 - Reconstruction Loss: 0.0939, \n",
      "Epoch 376 - Reconstruction Loss: 0.0812, \n",
      "Epoch 377 - Reconstruction Loss: 0.1042, \n",
      "Epoch 378 - Reconstruction Loss: 0.0879, \n",
      "Epoch 379 - Reconstruction Loss: 0.0950, \n",
      "Epoch 380 - Reconstruction Loss: 0.0834, \n",
      "Epoch 381 - Reconstruction Loss: 0.0871, \n",
      "Epoch 382 - Reconstruction Loss: 0.0930, \n",
      "Epoch 383 - Reconstruction Loss: 0.0901, \n",
      "Epoch 384 - Reconstruction Loss: 0.1000, \n",
      "Epoch 385 - Reconstruction Loss: 0.1245, \n",
      "Epoch 386 - Reconstruction Loss: 0.0869, \n",
      "Epoch 387 - Reconstruction Loss: 0.0936, \n",
      "Epoch 388 - Reconstruction Loss: 0.1178, \n",
      "Epoch 389 - Reconstruction Loss: 0.1021, \n",
      "Epoch 390 - Reconstruction Loss: 0.0851, \n",
      "Epoch 391 - Reconstruction Loss: 0.0918, \n",
      "Epoch 392 - Reconstruction Loss: 0.0948, \n",
      "Epoch 393 - Reconstruction Loss: 0.0928, \n",
      "Epoch 394 - Reconstruction Loss: 0.1076, \n",
      "Epoch 395 - Reconstruction Loss: 0.1081, \n",
      "Epoch 396 - Reconstruction Loss: 0.0947, \n",
      "Epoch 397 - Reconstruction Loss: 0.1037, \n",
      "Epoch 398 - Reconstruction Loss: 0.0976, \n",
      "Epoch 399 - Reconstruction Loss: 0.0883, \n",
      "Epoch 400 - Reconstruction Loss: 0.1161, \n",
      "Epoch 401 - Reconstruction Loss: 0.0987, \n",
      "Epoch 402 - Reconstruction Loss: 0.0802, \n",
      "Epoch 403 - Reconstruction Loss: 0.0945, \n",
      "Epoch 404 - Reconstruction Loss: 0.0966, \n",
      "Epoch 405 - Reconstruction Loss: 0.0845, \n",
      "Epoch 406 - Reconstruction Loss: 0.1062, \n",
      "Epoch 407 - Reconstruction Loss: 0.0745, \n",
      "Epoch 408 - Reconstruction Loss: 0.1167, \n",
      "Epoch 409 - Reconstruction Loss: 0.0954, \n",
      "Epoch 410 - Reconstruction Loss: 0.1096, \n",
      "Epoch 411 - Reconstruction Loss: 0.1108, \n",
      "Epoch 412 - Reconstruction Loss: 0.0855, \n",
      "Epoch 413 - Reconstruction Loss: 0.1097, \n",
      "Epoch 414 - Reconstruction Loss: 0.0894, \n",
      "Epoch 415 - Reconstruction Loss: 0.0840, \n",
      "Epoch 416 - Reconstruction Loss: 0.0801, \n",
      "Epoch 417 - Reconstruction Loss: 0.0977, \n",
      "Epoch 418 - Reconstruction Loss: 0.0897, \n",
      "Epoch 419 - Reconstruction Loss: 0.0968, \n",
      "Epoch 420 - Reconstruction Loss: 0.0878, \n",
      "Epoch 421 - Reconstruction Loss: 0.0782, \n",
      "Epoch 422 - Reconstruction Loss: 0.1033, \n",
      "Epoch 423 - Reconstruction Loss: 0.0999, \n",
      "Epoch 424 - Reconstruction Loss: 0.0906, \n",
      "Epoch 425 - Reconstruction Loss: 0.0949, \n",
      "Epoch 426 - Reconstruction Loss: 0.0788, \n",
      "Epoch 427 - Reconstruction Loss: 0.1164, \n",
      "Epoch 428 - Reconstruction Loss: 0.0785, \n",
      "Epoch 429 - Reconstruction Loss: 0.0951, \n",
      "Epoch 430 - Reconstruction Loss: 0.1050, \n",
      "Epoch 431 - Reconstruction Loss: 0.1076, \n",
      "Epoch 432 - Reconstruction Loss: 0.0889, \n",
      "Epoch 433 - Reconstruction Loss: 0.0987, \n",
      "Epoch 434 - Reconstruction Loss: 0.0945, \n",
      "Epoch 435 - Reconstruction Loss: 0.0905, \n",
      "Epoch 436 - Reconstruction Loss: 0.0967, \n",
      "Epoch 437 - Reconstruction Loss: 0.1050, \n",
      "Epoch 438 - Reconstruction Loss: 0.0913, \n",
      "Epoch 439 - Reconstruction Loss: 0.0922, \n",
      "Epoch 440 - Reconstruction Loss: 0.1026, \n",
      "Epoch 441 - Reconstruction Loss: 0.0873, \n",
      "Epoch 442 - Reconstruction Loss: 0.0917, \n",
      "Epoch 443 - Reconstruction Loss: 0.0916, \n",
      "Epoch 444 - Reconstruction Loss: 0.0928, \n",
      "Epoch 445 - Reconstruction Loss: 0.1079, \n",
      "Epoch 446 - Reconstruction Loss: 0.0949, \n",
      "Epoch 447 - Reconstruction Loss: 0.0991, \n",
      "Epoch 448 - Reconstruction Loss: 0.0787, \n",
      "Epoch 449 - Reconstruction Loss: 0.1027, \n",
      "Epoch 450 - Reconstruction Loss: 0.0838, \n",
      "Epoch 451 - Reconstruction Loss: 0.0903, \n",
      "Epoch 452 - Reconstruction Loss: 0.1030, \n",
      "Epoch 453 - Reconstruction Loss: 0.0921, \n",
      "Epoch 454 - Reconstruction Loss: 0.1014, \n",
      "Epoch 455 - Reconstruction Loss: 0.0780, \n",
      "Epoch 456 - Reconstruction Loss: 0.0851, \n",
      "Epoch 457 - Reconstruction Loss: 0.0971, \n",
      "Epoch 458 - Reconstruction Loss: 0.0924, \n",
      "Epoch 459 - Reconstruction Loss: 0.0957, \n",
      "Epoch 460 - Reconstruction Loss: 0.0845, \n",
      "Epoch 461 - Reconstruction Loss: 0.0860, \n",
      "Epoch 462 - Reconstruction Loss: 0.0828, \n",
      "Epoch 463 - Reconstruction Loss: 0.0844, \n",
      "Epoch 464 - Reconstruction Loss: 0.0961, \n",
      "Epoch 465 - Reconstruction Loss: 0.0925, \n",
      "Epoch 466 - Reconstruction Loss: 0.1205, \n",
      "Epoch 467 - Reconstruction Loss: 0.1092, \n",
      "Epoch 468 - Reconstruction Loss: 0.0843, \n",
      "Epoch 469 - Reconstruction Loss: 0.1061, \n",
      "Epoch 470 - Reconstruction Loss: 0.0967, \n",
      "Epoch 471 - Reconstruction Loss: 0.1100, \n",
      "Epoch 472 - Reconstruction Loss: 0.0757, \n",
      "Epoch 473 - Reconstruction Loss: 0.1004, \n",
      "Epoch 474 - Reconstruction Loss: 0.0945, \n",
      "Epoch 475 - Reconstruction Loss: 0.0879, \n",
      "Epoch 476 - Reconstruction Loss: 0.1016, \n",
      "Epoch 477 - Reconstruction Loss: 0.1030, \n",
      "Epoch 478 - Reconstruction Loss: 0.1016, \n",
      "Epoch 479 - Reconstruction Loss: 0.0848, \n",
      "Epoch 480 - Reconstruction Loss: 0.0680, \n",
      "Epoch 481 - Reconstruction Loss: 0.0825, \n",
      "Epoch 482 - Reconstruction Loss: 0.0689, \n",
      "Epoch 483 - Reconstruction Loss: 0.0907, \n",
      "Epoch 484 - Reconstruction Loss: 0.0978, \n",
      "Epoch 485 - Reconstruction Loss: 0.0778, \n",
      "Epoch 486 - Reconstruction Loss: 0.0958, \n",
      "Epoch 487 - Reconstruction Loss: 0.0869, \n",
      "Epoch 488 - Reconstruction Loss: 0.0950, \n",
      "Epoch 489 - Reconstruction Loss: 0.0747, \n",
      "Epoch 490 - Reconstruction Loss: 0.0880, \n",
      "Epoch 491 - Reconstruction Loss: 0.0942, \n",
      "Epoch 492 - Reconstruction Loss: 0.0882, \n",
      "Epoch 493 - Reconstruction Loss: 0.0792, \n",
      "Epoch 494 - Reconstruction Loss: 0.0998, \n",
      "Epoch 495 - Reconstruction Loss: 0.0858, \n",
      "Epoch 496 - Reconstruction Loss: 0.0879, \n",
      "Epoch 497 - Reconstruction Loss: 0.1046, \n",
      "Epoch 498 - Reconstruction Loss: 0.1050, \n",
      "Epoch 499 - Reconstruction Loss: 0.0801, \n",
      "Epoch 500 - Reconstruction Loss: 0.0720, \n",
      "Epoch 501 - Reconstruction Loss: 0.0962, \n",
      "Epoch 502 - Reconstruction Loss: 0.0916, \n",
      "Epoch 503 - Reconstruction Loss: 0.0998, \n",
      "Epoch 504 - Reconstruction Loss: 0.1108, \n",
      "Epoch 505 - Reconstruction Loss: 0.1010, \n",
      "Epoch 506 - Reconstruction Loss: 0.0994, \n",
      "Epoch 507 - Reconstruction Loss: 0.1010, \n",
      "Epoch 508 - Reconstruction Loss: 0.0769, \n",
      "Epoch 509 - Reconstruction Loss: 0.0943, \n",
      "Epoch 510 - Reconstruction Loss: 0.0949, \n",
      "Epoch 511 - Reconstruction Loss: 0.0897, \n",
      "Epoch 512 - Reconstruction Loss: 0.1040, \n",
      "Epoch 513 - Reconstruction Loss: 0.1096, \n",
      "Epoch 514 - Reconstruction Loss: 0.0841, \n",
      "Epoch 515 - Reconstruction Loss: 0.1013, \n",
      "Epoch 516 - Reconstruction Loss: 0.1032, \n",
      "Epoch 517 - Reconstruction Loss: 0.0910, \n",
      "Epoch 518 - Reconstruction Loss: 0.0859, \n",
      "Epoch 519 - Reconstruction Loss: 0.1026, \n",
      "Epoch 520 - Reconstruction Loss: 0.0823, \n",
      "Epoch 521 - Reconstruction Loss: 0.0735, \n",
      "Epoch 522 - Reconstruction Loss: 0.0794, \n",
      "Epoch 523 - Reconstruction Loss: 0.0903, \n",
      "Epoch 524 - Reconstruction Loss: 0.1108, \n",
      "Epoch 525 - Reconstruction Loss: 0.0968, \n",
      "Epoch 526 - Reconstruction Loss: 0.1002, \n",
      "Epoch 527 - Reconstruction Loss: 0.0989, \n",
      "Epoch 528 - Reconstruction Loss: 0.0851, \n",
      "Epoch 529 - Reconstruction Loss: 0.1076, \n",
      "Epoch 530 - Reconstruction Loss: 0.0903, \n",
      "Epoch 531 - Reconstruction Loss: 0.0906, \n",
      "Epoch 532 - Reconstruction Loss: 0.1079, \n",
      "Epoch 533 - Reconstruction Loss: 0.1061, \n",
      "Epoch 534 - Reconstruction Loss: 0.0827, \n",
      "Epoch 535 - Reconstruction Loss: 0.1043, \n",
      "Epoch 536 - Reconstruction Loss: 0.1004, \n",
      "Epoch 537 - Reconstruction Loss: 0.1163, \n",
      "Epoch 538 - Reconstruction Loss: 0.1103, \n",
      "Epoch 539 - Reconstruction Loss: 0.0897, \n",
      "Epoch 540 - Reconstruction Loss: 0.1041, \n",
      "Epoch 541 - Reconstruction Loss: 0.0878, \n",
      "Epoch 542 - Reconstruction Loss: 0.0894, \n",
      "Epoch 543 - Reconstruction Loss: 0.0939, \n",
      "Epoch 544 - Reconstruction Loss: 0.0887, \n",
      "Epoch 545 - Reconstruction Loss: 0.0915, \n",
      "Epoch 546 - Reconstruction Loss: 0.0920, \n",
      "Epoch 547 - Reconstruction Loss: 0.1083, \n",
      "Epoch 548 - Reconstruction Loss: 0.0903, \n",
      "Epoch 549 - Reconstruction Loss: 0.0998, \n",
      "Epoch 550 - Reconstruction Loss: 0.0944, \n",
      "Epoch 551 - Reconstruction Loss: 0.0895, \n",
      "Epoch 552 - Reconstruction Loss: 0.0818, \n",
      "Epoch 553 - Reconstruction Loss: 0.0872, \n",
      "Epoch 554 - Reconstruction Loss: 0.0920, \n",
      "Epoch 555 - Reconstruction Loss: 0.0818, \n",
      "Epoch 556 - Reconstruction Loss: 0.1001, \n",
      "Epoch 557 - Reconstruction Loss: 0.1076, \n",
      "Epoch 558 - Reconstruction Loss: 0.0871, \n",
      "Epoch 559 - Reconstruction Loss: 0.0909, \n",
      "Epoch 560 - Reconstruction Loss: 0.1077, \n",
      "Epoch 561 - Reconstruction Loss: 0.0959, \n",
      "Epoch 562 - Reconstruction Loss: 0.0797, \n",
      "Epoch 563 - Reconstruction Loss: 0.0917, \n",
      "Epoch 564 - Reconstruction Loss: 0.1000, \n",
      "Epoch 565 - Reconstruction Loss: 0.0862, \n",
      "Epoch 566 - Reconstruction Loss: 0.1159, \n",
      "Epoch 567 - Reconstruction Loss: 0.1060, \n",
      "Epoch 568 - Reconstruction Loss: 0.0942, \n",
      "Epoch 569 - Reconstruction Loss: 0.0869, \n",
      "Epoch 570 - Reconstruction Loss: 0.1015, \n",
      "Epoch 571 - Reconstruction Loss: 0.1081, \n",
      "Epoch 572 - Reconstruction Loss: 0.1075, \n",
      "Epoch 573 - Reconstruction Loss: 0.1054, \n",
      "Epoch 574 - Reconstruction Loss: 0.0793, \n",
      "Epoch 575 - Reconstruction Loss: 0.0913, \n",
      "Epoch 576 - Reconstruction Loss: 0.0797, \n",
      "Epoch 577 - Reconstruction Loss: 0.0988, \n",
      "Epoch 578 - Reconstruction Loss: 0.0911, \n",
      "Epoch 579 - Reconstruction Loss: 0.0900, \n",
      "Epoch 580 - Reconstruction Loss: 0.0975, \n",
      "Epoch 581 - Reconstruction Loss: 0.0765, \n",
      "Epoch 582 - Reconstruction Loss: 0.1073, \n",
      "Epoch 583 - Reconstruction Loss: 0.0880, \n",
      "Epoch 584 - Reconstruction Loss: 0.1103, \n",
      "Epoch 585 - Reconstruction Loss: 0.0898, \n",
      "Epoch 586 - Reconstruction Loss: 0.1009, \n",
      "Epoch 587 - Reconstruction Loss: 0.0933, \n",
      "Epoch 588 - Reconstruction Loss: 0.0838, \n",
      "Epoch 589 - Reconstruction Loss: 0.1029, \n",
      "Epoch 590 - Reconstruction Loss: 0.0799, \n",
      "Epoch 591 - Reconstruction Loss: 0.0909, \n",
      "Epoch 592 - Reconstruction Loss: 0.1102, \n",
      "Epoch 593 - Reconstruction Loss: 0.0893, \n",
      "Epoch 594 - Reconstruction Loss: 0.0892, \n",
      "Epoch 595 - Reconstruction Loss: 0.0778, \n",
      "Epoch 596 - Reconstruction Loss: 0.1183, \n",
      "Epoch 597 - Reconstruction Loss: 0.1032, \n",
      "Epoch 598 - Reconstruction Loss: 0.0986, \n",
      "Epoch 599 - Reconstruction Loss: 0.1044, \n",
      "Epoch 600 - Reconstruction Loss: 0.0988, \n",
      "Epoch 601 - Reconstruction Loss: 0.0934, \n",
      "Epoch 602 - Reconstruction Loss: 0.0970, \n",
      "Epoch 603 - Reconstruction Loss: 0.1046, \n",
      "Epoch 604 - Reconstruction Loss: 0.0987, \n",
      "Epoch 605 - Reconstruction Loss: 0.0921, \n",
      "Epoch 606 - Reconstruction Loss: 0.0935, \n",
      "Epoch 607 - Reconstruction Loss: 0.0793, \n",
      "Epoch 608 - Reconstruction Loss: 0.1289, \n",
      "Epoch 609 - Reconstruction Loss: 0.0937, \n",
      "Epoch 610 - Reconstruction Loss: 0.0756, \n",
      "Epoch 611 - Reconstruction Loss: 0.0874, \n",
      "Epoch 612 - Reconstruction Loss: 0.0970, \n",
      "Epoch 613 - Reconstruction Loss: 0.0767, \n",
      "Epoch 614 - Reconstruction Loss: 0.1079, \n",
      "Epoch 615 - Reconstruction Loss: 0.0868, \n",
      "Epoch 616 - Reconstruction Loss: 0.0995, \n",
      "Epoch 617 - Reconstruction Loss: 0.0860, \n",
      "Epoch 618 - Reconstruction Loss: 0.0776, \n",
      "Epoch 619 - Reconstruction Loss: 0.1154, \n",
      "Epoch 620 - Reconstruction Loss: 0.1022, \n",
      "Epoch 621 - Reconstruction Loss: 0.0988, \n",
      "Epoch 622 - Reconstruction Loss: 0.1129, \n",
      "Epoch 623 - Reconstruction Loss: 0.0949, \n",
      "Epoch 624 - Reconstruction Loss: 0.1030, \n",
      "Epoch 625 - Reconstruction Loss: 0.0923, \n",
      "Epoch 626 - Reconstruction Loss: 0.0908, \n",
      "Epoch 627 - Reconstruction Loss: 0.0769, \n",
      "Epoch 628 - Reconstruction Loss: 0.1150, \n",
      "Epoch 629 - Reconstruction Loss: 0.1164, \n",
      "Epoch 630 - Reconstruction Loss: 0.0816, \n",
      "Epoch 631 - Reconstruction Loss: 0.0936, \n",
      "Epoch 632 - Reconstruction Loss: 0.0958, \n",
      "Epoch 633 - Reconstruction Loss: 0.0935, \n",
      "Epoch 634 - Reconstruction Loss: 0.1259, \n",
      "Epoch 635 - Reconstruction Loss: 0.0735, \n",
      "Epoch 636 - Reconstruction Loss: 0.1023, \n",
      "Epoch 637 - Reconstruction Loss: 0.1104, \n",
      "Epoch 638 - Reconstruction Loss: 0.0740, \n",
      "Epoch 639 - Reconstruction Loss: 0.0879, \n",
      "Epoch 640 - Reconstruction Loss: 0.0874, \n",
      "Epoch 641 - Reconstruction Loss: 0.1038, \n",
      "Epoch 642 - Reconstruction Loss: 0.1007, \n",
      "Epoch 643 - Reconstruction Loss: 0.0893, \n",
      "Epoch 644 - Reconstruction Loss: 0.0899, \n",
      "Epoch 645 - Reconstruction Loss: 0.0904, \n",
      "Epoch 646 - Reconstruction Loss: 0.0837, \n",
      "Epoch 647 - Reconstruction Loss: 0.0980, \n",
      "Epoch 648 - Reconstruction Loss: 0.0947, \n",
      "Epoch 649 - Reconstruction Loss: 0.1056, \n",
      "Epoch 650 - Reconstruction Loss: 0.1056, \n",
      "Epoch 651 - Reconstruction Loss: 0.1115, \n",
      "Epoch 652 - Reconstruction Loss: 0.0891, \n",
      "Epoch 653 - Reconstruction Loss: 0.1213, \n",
      "Epoch 654 - Reconstruction Loss: 0.1034, \n",
      "Epoch 655 - Reconstruction Loss: 0.0878, \n",
      "Epoch 656 - Reconstruction Loss: 0.0849, \n",
      "Epoch 657 - Reconstruction Loss: 0.0918, \n",
      "Epoch 658 - Reconstruction Loss: 0.0948, \n",
      "Epoch 659 - Reconstruction Loss: 0.1115, \n",
      "Epoch 660 - Reconstruction Loss: 0.0921, \n",
      "Epoch 661 - Reconstruction Loss: 0.0983, \n",
      "Epoch 662 - Reconstruction Loss: 0.1120, \n",
      "Epoch 663 - Reconstruction Loss: 0.0969, \n",
      "Epoch 664 - Reconstruction Loss: 0.0925, \n",
      "Epoch 665 - Reconstruction Loss: 0.0693, \n",
      "Epoch 666 - Reconstruction Loss: 0.1004, \n",
      "Epoch 667 - Reconstruction Loss: 0.0911, \n",
      "Epoch 668 - Reconstruction Loss: 0.1059, \n",
      "Epoch 669 - Reconstruction Loss: 0.1017, \n",
      "Epoch 670 - Reconstruction Loss: 0.0835, \n",
      "Epoch 671 - Reconstruction Loss: 0.0981, \n",
      "Epoch 672 - Reconstruction Loss: 0.0972, \n",
      "Epoch 673 - Reconstruction Loss: 0.1039, \n",
      "Epoch 674 - Reconstruction Loss: 0.0948, \n",
      "Epoch 675 - Reconstruction Loss: 0.0866, \n",
      "Epoch 676 - Reconstruction Loss: 0.0903, \n",
      "Epoch 677 - Reconstruction Loss: 0.0912, \n",
      "Epoch 678 - Reconstruction Loss: 0.0727, \n",
      "Epoch 679 - Reconstruction Loss: 0.1055, \n",
      "Epoch 680 - Reconstruction Loss: 0.0952, \n",
      "Epoch 681 - Reconstruction Loss: 0.0765, \n",
      "Epoch 682 - Reconstruction Loss: 0.1028, \n",
      "Epoch 683 - Reconstruction Loss: 0.0878, \n",
      "Epoch 684 - Reconstruction Loss: 0.0692, \n",
      "Epoch 685 - Reconstruction Loss: 0.0959, \n",
      "Epoch 686 - Reconstruction Loss: 0.0844, \n",
      "Epoch 687 - Reconstruction Loss: 0.0708, \n",
      "Epoch 688 - Reconstruction Loss: 0.0790, \n",
      "Epoch 689 - Reconstruction Loss: 0.0869, \n",
      "Epoch 690 - Reconstruction Loss: 0.1066, \n",
      "Epoch 691 - Reconstruction Loss: 0.0837, \n",
      "Epoch 692 - Reconstruction Loss: 0.0885, \n",
      "Epoch 693 - Reconstruction Loss: 0.0859, \n",
      "Epoch 694 - Reconstruction Loss: 0.1027, \n",
      "Epoch 695 - Reconstruction Loss: 0.1004, \n",
      "Epoch 696 - Reconstruction Loss: 0.1059, \n",
      "Epoch 697 - Reconstruction Loss: 0.0728, \n",
      "Epoch 698 - Reconstruction Loss: 0.0807, \n",
      "Epoch 699 - Reconstruction Loss: 0.0577, \n",
      "Model saved at 0.058\n",
      "Epoch 700 - Reconstruction Loss: 0.1069, \n",
      "Epoch 701 - Reconstruction Loss: 0.0967, \n",
      "Epoch 702 - Reconstruction Loss: 0.0905, \n",
      "Epoch 703 - Reconstruction Loss: 0.0837, \n",
      "Epoch 704 - Reconstruction Loss: 0.0989, \n",
      "Epoch 705 - Reconstruction Loss: 0.0956, \n",
      "Epoch 706 - Reconstruction Loss: 0.0806, \n",
      "Epoch 707 - Reconstruction Loss: 0.0900, \n",
      "Epoch 708 - Reconstruction Loss: 0.0974, \n",
      "Epoch 709 - Reconstruction Loss: 0.0918, \n",
      "Epoch 710 - Reconstruction Loss: 0.0830, \n",
      "Epoch 711 - Reconstruction Loss: 0.0976, \n",
      "Epoch 712 - Reconstruction Loss: 0.1117, \n",
      "Epoch 713 - Reconstruction Loss: 0.0943, \n",
      "Epoch 714 - Reconstruction Loss: 0.0806, \n",
      "Epoch 715 - Reconstruction Loss: 0.1321, \n",
      "Epoch 716 - Reconstruction Loss: 0.1129, \n",
      "Epoch 717 - Reconstruction Loss: 0.1004, \n",
      "Epoch 718 - Reconstruction Loss: 0.0914, \n",
      "Epoch 719 - Reconstruction Loss: 0.0971, \n",
      "Epoch 720 - Reconstruction Loss: 0.0875, \n",
      "Epoch 721 - Reconstruction Loss: 0.0947, \n",
      "Epoch 722 - Reconstruction Loss: 0.1009, \n",
      "Epoch 723 - Reconstruction Loss: 0.0800, \n",
      "Epoch 724 - Reconstruction Loss: 0.0967, \n",
      "Epoch 725 - Reconstruction Loss: 0.1072, \n",
      "Epoch 726 - Reconstruction Loss: 0.0941, \n",
      "Epoch 727 - Reconstruction Loss: 0.1025, \n",
      "Epoch 728 - Reconstruction Loss: 0.0942, \n",
      "Epoch 729 - Reconstruction Loss: 0.0934, \n",
      "Epoch 730 - Reconstruction Loss: 0.0899, \n",
      "Epoch 731 - Reconstruction Loss: 0.0955, \n",
      "Epoch 732 - Reconstruction Loss: 0.0880, \n",
      "Epoch 733 - Reconstruction Loss: 0.0893, \n",
      "Epoch 734 - Reconstruction Loss: 0.1092, \n",
      "Epoch 735 - Reconstruction Loss: 0.0921, \n",
      "Epoch 736 - Reconstruction Loss: 0.0905, \n",
      "Epoch 737 - Reconstruction Loss: 0.0815, \n",
      "Epoch 738 - Reconstruction Loss: 0.0852, \n",
      "Epoch 739 - Reconstruction Loss: 0.0934, \n",
      "Epoch 740 - Reconstruction Loss: 0.1013, \n",
      "Epoch 741 - Reconstruction Loss: 0.0875, \n",
      "Epoch 742 - Reconstruction Loss: 0.1094, \n",
      "Epoch 743 - Reconstruction Loss: 0.0894, \n",
      "Epoch 744 - Reconstruction Loss: 0.1148, \n",
      "Epoch 745 - Reconstruction Loss: 0.0800, \n",
      "Epoch 746 - Reconstruction Loss: 0.0872, \n",
      "Epoch 747 - Reconstruction Loss: 0.1113, \n",
      "Epoch 748 - Reconstruction Loss: 0.0904, \n",
      "Epoch 749 - Reconstruction Loss: 0.0939, \n",
      "Epoch 750 - Reconstruction Loss: 0.0831, \n",
      "Epoch 751 - Reconstruction Loss: 0.1012, \n",
      "Epoch 752 - Reconstruction Loss: 0.1028, \n",
      "Epoch 753 - Reconstruction Loss: 0.0904, \n",
      "Epoch 754 - Reconstruction Loss: 0.0790, \n",
      "Epoch 755 - Reconstruction Loss: 0.0917, \n",
      "Epoch 756 - Reconstruction Loss: 0.0842, \n",
      "Epoch 757 - Reconstruction Loss: 0.1099, \n",
      "Epoch 758 - Reconstruction Loss: 0.0904, \n",
      "Epoch 759 - Reconstruction Loss: 0.1019, \n",
      "Epoch 760 - Reconstruction Loss: 0.0917, \n",
      "Epoch 761 - Reconstruction Loss: 0.0759, \n",
      "Epoch 762 - Reconstruction Loss: 0.1051, \n",
      "Epoch 763 - Reconstruction Loss: 0.1064, \n",
      "Epoch 764 - Reconstruction Loss: 0.0875, \n",
      "Epoch 765 - Reconstruction Loss: 0.0915, \n",
      "Epoch 766 - Reconstruction Loss: 0.1119, \n",
      "Epoch 767 - Reconstruction Loss: 0.0874, \n",
      "Epoch 768 - Reconstruction Loss: 0.0772, \n",
      "Epoch 769 - Reconstruction Loss: 0.0941, \n",
      "Epoch 770 - Reconstruction Loss: 0.0930, \n",
      "Epoch 771 - Reconstruction Loss: 0.1028, \n",
      "Epoch 772 - Reconstruction Loss: 0.0987, \n",
      "Epoch 773 - Reconstruction Loss: 0.1012, \n",
      "Epoch 774 - Reconstruction Loss: 0.0960, \n",
      "Epoch 775 - Reconstruction Loss: 0.0873, \n",
      "Epoch 776 - Reconstruction Loss: 0.0939, \n",
      "Epoch 777 - Reconstruction Loss: 0.1018, \n",
      "Epoch 778 - Reconstruction Loss: 0.0922, \n",
      "Epoch 779 - Reconstruction Loss: 0.0827, \n",
      "Epoch 780 - Reconstruction Loss: 0.1012, \n",
      "Epoch 781 - Reconstruction Loss: 0.0961, \n",
      "Epoch 782 - Reconstruction Loss: 0.0816, \n",
      "Epoch 783 - Reconstruction Loss: 0.0944, \n",
      "Epoch 784 - Reconstruction Loss: 0.0903, \n",
      "Epoch 785 - Reconstruction Loss: 0.0881, \n",
      "Epoch 786 - Reconstruction Loss: 0.0805, \n",
      "Epoch 787 - Reconstruction Loss: 0.1117, \n",
      "Epoch 788 - Reconstruction Loss: 0.1001, \n",
      "Epoch 789 - Reconstruction Loss: 0.0951, \n",
      "Epoch 790 - Reconstruction Loss: 0.0981, \n",
      "Epoch 791 - Reconstruction Loss: 0.0846, \n",
      "Epoch 792 - Reconstruction Loss: 0.1053, \n",
      "Epoch 793 - Reconstruction Loss: 0.1177, \n",
      "Epoch 794 - Reconstruction Loss: 0.0799, \n",
      "Epoch 795 - Reconstruction Loss: 0.0831, \n",
      "Epoch 796 - Reconstruction Loss: 0.0838, \n",
      "Epoch 797 - Reconstruction Loss: 0.0884, \n",
      "Epoch 798 - Reconstruction Loss: 0.0968, \n",
      "Epoch 799 - Reconstruction Loss: 0.0814, \n",
      "Epoch 800 - Reconstruction Loss: 0.0973, \n",
      "Epoch 801 - Reconstruction Loss: 0.0787, \n",
      "Epoch 802 - Reconstruction Loss: 0.0865, \n",
      "Epoch 803 - Reconstruction Loss: 0.1084, \n",
      "Epoch 804 - Reconstruction Loss: 0.0868, \n",
      "Epoch 805 - Reconstruction Loss: 0.0942, \n",
      "Epoch 806 - Reconstruction Loss: 0.0901, \n",
      "Epoch 807 - Reconstruction Loss: 0.1048, \n",
      "Epoch 808 - Reconstruction Loss: 0.0960, \n",
      "Epoch 809 - Reconstruction Loss: 0.0880, \n",
      "Epoch 810 - Reconstruction Loss: 0.0814, \n",
      "Epoch 811 - Reconstruction Loss: 0.0920, \n",
      "Epoch 812 - Reconstruction Loss: 0.0904, \n",
      "Epoch 813 - Reconstruction Loss: 0.0962, \n",
      "Epoch 814 - Reconstruction Loss: 0.1033, \n",
      "Epoch 815 - Reconstruction Loss: 0.1084, \n",
      "Epoch 816 - Reconstruction Loss: 0.1107, \n",
      "Epoch 817 - Reconstruction Loss: 0.0870, \n",
      "Epoch 818 - Reconstruction Loss: 0.1100, \n",
      "Epoch 819 - Reconstruction Loss: 0.0829, \n",
      "Epoch 820 - Reconstruction Loss: 0.0951, \n",
      "Epoch 821 - Reconstruction Loss: 0.1033, \n",
      "Epoch 822 - Reconstruction Loss: 0.0848, \n",
      "Epoch 823 - Reconstruction Loss: 0.0933, \n",
      "Epoch 824 - Reconstruction Loss: 0.1023, \n",
      "Epoch 825 - Reconstruction Loss: 0.0809, \n",
      "Epoch 826 - Reconstruction Loss: 0.0854, \n",
      "Epoch 827 - Reconstruction Loss: 0.1043, \n",
      "Epoch 828 - Reconstruction Loss: 0.0779, \n",
      "Epoch 829 - Reconstruction Loss: 0.1000, \n",
      "Epoch 830 - Reconstruction Loss: 0.0958, \n",
      "Epoch 831 - Reconstruction Loss: 0.1062, \n",
      "Epoch 832 - Reconstruction Loss: 0.0928, \n",
      "Epoch 833 - Reconstruction Loss: 0.0915, \n",
      "Epoch 834 - Reconstruction Loss: 0.1074, \n",
      "Epoch 835 - Reconstruction Loss: 0.0906, \n",
      "Epoch 836 - Reconstruction Loss: 0.0792, \n",
      "Epoch 837 - Reconstruction Loss: 0.0999, \n",
      "Epoch 838 - Reconstruction Loss: 0.0856, \n",
      "Epoch 839 - Reconstruction Loss: 0.0990, \n",
      "Epoch 840 - Reconstruction Loss: 0.0801, \n",
      "Epoch 841 - Reconstruction Loss: 0.0982, \n",
      "Epoch 842 - Reconstruction Loss: 0.0969, \n",
      "Epoch 843 - Reconstruction Loss: 0.0963, \n",
      "Epoch 844 - Reconstruction Loss: 0.0775, \n",
      "Epoch 845 - Reconstruction Loss: 0.1044, \n",
      "Epoch 846 - Reconstruction Loss: 0.0872, \n",
      "Epoch 847 - Reconstruction Loss: 0.1027, \n",
      "Epoch 848 - Reconstruction Loss: 0.1062, \n",
      "Epoch 849 - Reconstruction Loss: 0.1093, \n",
      "Epoch 850 - Reconstruction Loss: 0.0847, \n",
      "Epoch 851 - Reconstruction Loss: 0.0881, \n",
      "Epoch 852 - Reconstruction Loss: 0.0705, \n",
      "Epoch 853 - Reconstruction Loss: 0.0849, \n",
      "Epoch 854 - Reconstruction Loss: 0.0854, \n",
      "Epoch 855 - Reconstruction Loss: 0.0923, \n",
      "Epoch 856 - Reconstruction Loss: 0.0848, \n",
      "Epoch 857 - Reconstruction Loss: 0.0841, \n",
      "Epoch 858 - Reconstruction Loss: 0.0912, \n",
      "Epoch 859 - Reconstruction Loss: 0.0974, \n",
      "Epoch 860 - Reconstruction Loss: 0.0934, \n",
      "Epoch 861 - Reconstruction Loss: 0.1043, \n",
      "Epoch 862 - Reconstruction Loss: 0.0937, \n",
      "Epoch 863 - Reconstruction Loss: 0.1106, \n",
      "Epoch 864 - Reconstruction Loss: 0.0844, \n",
      "Epoch 865 - Reconstruction Loss: 0.0961, \n",
      "Epoch 866 - Reconstruction Loss: 0.0758, \n",
      "Epoch 867 - Reconstruction Loss: 0.0932, \n",
      "Epoch 868 - Reconstruction Loss: 0.0976, \n",
      "Epoch 869 - Reconstruction Loss: 0.1102, \n",
      "Epoch 870 - Reconstruction Loss: 0.0895, \n",
      "Epoch 871 - Reconstruction Loss: 0.1052, \n",
      "Epoch 872 - Reconstruction Loss: 0.1199, \n",
      "Epoch 873 - Reconstruction Loss: 0.0934, \n",
      "Epoch 874 - Reconstruction Loss: 0.0913, \n",
      "Epoch 875 - Reconstruction Loss: 0.1029, \n",
      "Epoch 876 - Reconstruction Loss: 0.0750, \n",
      "Epoch 877 - Reconstruction Loss: 0.1037, \n",
      "Epoch 878 - Reconstruction Loss: 0.0835, \n",
      "Epoch 879 - Reconstruction Loss: 0.0785, \n",
      "Epoch 880 - Reconstruction Loss: 0.1116, \n",
      "Epoch 881 - Reconstruction Loss: 0.1051, \n",
      "Epoch 882 - Reconstruction Loss: 0.1131, \n",
      "Epoch 883 - Reconstruction Loss: 0.0789, \n",
      "Epoch 884 - Reconstruction Loss: 0.0863, \n",
      "Epoch 885 - Reconstruction Loss: 0.0744, \n",
      "Epoch 886 - Reconstruction Loss: 0.1076, \n",
      "Epoch 887 - Reconstruction Loss: 0.1175, \n",
      "Epoch 888 - Reconstruction Loss: 0.1210, \n",
      "Epoch 889 - Reconstruction Loss: 0.0865, \n",
      "Epoch 890 - Reconstruction Loss: 0.0840, \n",
      "Epoch 891 - Reconstruction Loss: 0.0728, \n",
      "Epoch 892 - Reconstruction Loss: 0.0913, \n",
      "Epoch 893 - Reconstruction Loss: 0.0892, \n",
      "Epoch 894 - Reconstruction Loss: 0.0750, \n",
      "Epoch 895 - Reconstruction Loss: 0.0851, \n",
      "Epoch 896 - Reconstruction Loss: 0.1082, \n",
      "Epoch 897 - Reconstruction Loss: 0.0861, \n",
      "Epoch 898 - Reconstruction Loss: 0.1000, \n",
      "Epoch 899 - Reconstruction Loss: 0.0959, \n",
      "Epoch 900 - Reconstruction Loss: 0.0990, \n",
      "Epoch 901 - Reconstruction Loss: 0.1074, \n",
      "Epoch 902 - Reconstruction Loss: 0.0882, \n",
      "Epoch 903 - Reconstruction Loss: 0.1045, \n",
      "Epoch 904 - Reconstruction Loss: 0.0894, \n",
      "Epoch 905 - Reconstruction Loss: 0.1027, \n",
      "Epoch 906 - Reconstruction Loss: 0.0871, \n",
      "Epoch 907 - Reconstruction Loss: 0.0867, \n",
      "Epoch 908 - Reconstruction Loss: 0.1189, \n",
      "Epoch 909 - Reconstruction Loss: 0.1156, \n",
      "Epoch 910 - Reconstruction Loss: 0.1199, \n",
      "Epoch 911 - Reconstruction Loss: 0.1074, \n",
      "Epoch 912 - Reconstruction Loss: 0.0820, \n",
      "Epoch 913 - Reconstruction Loss: 0.1037, \n",
      "Epoch 914 - Reconstruction Loss: 0.0952, \n",
      "Epoch 915 - Reconstruction Loss: 0.1008, \n",
      "Epoch 916 - Reconstruction Loss: 0.0939, \n",
      "Epoch 917 - Reconstruction Loss: 0.1003, \n",
      "Epoch 918 - Reconstruction Loss: 0.0757, \n",
      "Epoch 919 - Reconstruction Loss: 0.1021, \n",
      "Epoch 920 - Reconstruction Loss: 0.0800, \n",
      "Epoch 921 - Reconstruction Loss: 0.1068, \n",
      "Epoch 922 - Reconstruction Loss: 0.1065, \n",
      "Epoch 923 - Reconstruction Loss: 0.0963, \n",
      "Epoch 924 - Reconstruction Loss: 0.0952, \n",
      "Epoch 925 - Reconstruction Loss: 0.0949, \n",
      "Epoch 926 - Reconstruction Loss: 0.0996, \n",
      "Epoch 927 - Reconstruction Loss: 0.0757, \n",
      "Epoch 928 - Reconstruction Loss: 0.0899, \n",
      "Epoch 929 - Reconstruction Loss: 0.1007, \n",
      "Epoch 930 - Reconstruction Loss: 0.0838, \n",
      "Epoch 931 - Reconstruction Loss: 0.0955, \n",
      "Epoch 932 - Reconstruction Loss: 0.0836, \n",
      "Epoch 933 - Reconstruction Loss: 0.0934, \n",
      "Epoch 934 - Reconstruction Loss: 0.0989, \n",
      "Epoch 935 - Reconstruction Loss: 0.0925, \n",
      "Epoch 936 - Reconstruction Loss: 0.0795, \n",
      "Epoch 937 - Reconstruction Loss: 0.0899, \n",
      "Epoch 938 - Reconstruction Loss: 0.0895, \n",
      "Epoch 939 - Reconstruction Loss: 0.1180, \n",
      "Epoch 940 - Reconstruction Loss: 0.0943, \n",
      "Epoch 941 - Reconstruction Loss: 0.0914, \n",
      "Epoch 942 - Reconstruction Loss: 0.0987, \n",
      "Epoch 943 - Reconstruction Loss: 0.1039, \n",
      "Epoch 944 - Reconstruction Loss: 0.0840, \n",
      "Epoch 945 - Reconstruction Loss: 0.1052, \n",
      "Epoch 946 - Reconstruction Loss: 0.1019, \n",
      "Epoch 947 - Reconstruction Loss: 0.0813, \n",
      "Epoch 948 - Reconstruction Loss: 0.0942, \n",
      "Epoch 949 - Reconstruction Loss: 0.0994, \n",
      "Epoch 950 - Reconstruction Loss: 0.0913, \n",
      "Epoch 951 - Reconstruction Loss: 0.0898, \n",
      "Epoch 952 - Reconstruction Loss: 0.1148, \n",
      "Epoch 953 - Reconstruction Loss: 0.0797, \n",
      "Epoch 954 - Reconstruction Loss: 0.1021, \n",
      "Epoch 955 - Reconstruction Loss: 0.1052, \n",
      "Epoch 956 - Reconstruction Loss: 0.1010, \n",
      "Epoch 957 - Reconstruction Loss: 0.0943, \n",
      "Epoch 958 - Reconstruction Loss: 0.0766, \n",
      "Epoch 959 - Reconstruction Loss: 0.1142, \n",
      "Epoch 960 - Reconstruction Loss: 0.0865, \n",
      "Epoch 961 - Reconstruction Loss: 0.0933, \n",
      "Epoch 962 - Reconstruction Loss: 0.0893, \n",
      "Epoch 963 - Reconstruction Loss: 0.0893, \n",
      "Epoch 964 - Reconstruction Loss: 0.0950, \n",
      "Epoch 965 - Reconstruction Loss: 0.0765, \n",
      "Epoch 966 - Reconstruction Loss: 0.0960, \n",
      "Epoch 967 - Reconstruction Loss: 0.0837, \n",
      "Epoch 968 - Reconstruction Loss: 0.0803, \n",
      "Epoch 969 - Reconstruction Loss: 0.0988, \n",
      "Epoch 970 - Reconstruction Loss: 0.1094, \n",
      "Epoch 971 - Reconstruction Loss: 0.0843, \n",
      "Epoch 972 - Reconstruction Loss: 0.0853, \n",
      "Epoch 973 - Reconstruction Loss: 0.0979, \n",
      "Epoch 974 - Reconstruction Loss: 0.1059, \n",
      "Epoch 975 - Reconstruction Loss: 0.0916, \n",
      "Epoch 976 - Reconstruction Loss: 0.0949, \n",
      "Epoch 977 - Reconstruction Loss: 0.0968, \n",
      "Epoch 978 - Reconstruction Loss: 0.0977, \n",
      "Epoch 979 - Reconstruction Loss: 0.0990, \n",
      "Epoch 980 - Reconstruction Loss: 0.0994, \n",
      "Epoch 981 - Reconstruction Loss: 0.0936, \n",
      "Epoch 982 - Reconstruction Loss: 0.0923, \n",
      "Epoch 983 - Reconstruction Loss: 0.0852, \n",
      "Epoch 984 - Reconstruction Loss: 0.0769, \n",
      "Epoch 985 - Reconstruction Loss: 0.1073, \n",
      "Epoch 986 - Reconstruction Loss: 0.0811, \n",
      "Epoch 987 - Reconstruction Loss: 0.0946, \n",
      "Epoch 988 - Reconstruction Loss: 0.0833, \n",
      "Epoch 989 - Reconstruction Loss: 0.0969, \n",
      "Epoch 990 - Reconstruction Loss: 0.1024, \n",
      "Epoch 991 - Reconstruction Loss: 0.0775, \n",
      "Epoch 992 - Reconstruction Loss: 0.1096, \n",
      "Epoch 993 - Reconstruction Loss: 0.0938, \n",
      "Epoch 994 - Reconstruction Loss: 0.1273, \n",
      "Epoch 995 - Reconstruction Loss: 0.0965, \n",
      "Epoch 996 - Reconstruction Loss: 0.0892, \n",
      "Epoch 997 - Reconstruction Loss: 0.1054, \n",
      "Epoch 998 - Reconstruction Loss: 0.0839, \n",
      "Epoch 999 - Reconstruction Loss: 0.1009, \n",
      "Epoch 1000 - Reconstruction Loss: 0.1048, \n",
      "Epoch 1001 - Reconstruction Loss: 0.0989, \n",
      "Epoch 1002 - Reconstruction Loss: 0.0942, \n",
      "Epoch 1003 - Reconstruction Loss: 0.0975, \n",
      "Epoch 1004 - Reconstruction Loss: 0.1096, \n",
      "Epoch 1005 - Reconstruction Loss: 0.0961, \n",
      "Epoch 1006 - Reconstruction Loss: 0.1074, \n",
      "Epoch 1007 - Reconstruction Loss: 0.0856, \n",
      "Epoch 1008 - Reconstruction Loss: 0.1026, \n",
      "Epoch 1009 - Reconstruction Loss: 0.0675, \n",
      "Epoch 1010 - Reconstruction Loss: 0.1100, \n",
      "Epoch 1011 - Reconstruction Loss: 0.0951, \n",
      "Epoch 1012 - Reconstruction Loss: 0.1027, \n",
      "Epoch 1013 - Reconstruction Loss: 0.0986, \n",
      "Epoch 1014 - Reconstruction Loss: 0.0916, \n",
      "Epoch 1015 - Reconstruction Loss: 0.0819, \n",
      "Epoch 1016 - Reconstruction Loss: 0.0809, \n",
      "Epoch 1017 - Reconstruction Loss: 0.1013, \n",
      "Epoch 1018 - Reconstruction Loss: 0.0888, \n",
      "Epoch 1019 - Reconstruction Loss: 0.0898, \n",
      "Epoch 1020 - Reconstruction Loss: 0.1095, \n",
      "Epoch 1021 - Reconstruction Loss: 0.0876, \n",
      "Epoch 1022 - Reconstruction Loss: 0.0991, \n",
      "Epoch 1023 - Reconstruction Loss: 0.1167, \n",
      "Epoch 1024 - Reconstruction Loss: 0.0794, \n",
      "Epoch 1025 - Reconstruction Loss: 0.0814, \n",
      "Epoch 1026 - Reconstruction Loss: 0.1020, \n",
      "Epoch 1027 - Reconstruction Loss: 0.0807, \n",
      "Epoch 1028 - Reconstruction Loss: 0.0969, \n",
      "Epoch 1029 - Reconstruction Loss: 0.1040, \n",
      "Epoch 1030 - Reconstruction Loss: 0.1000, \n",
      "Epoch 1031 - Reconstruction Loss: 0.0661, \n",
      "Epoch 1032 - Reconstruction Loss: 0.0911, \n",
      "Epoch 1033 - Reconstruction Loss: 0.0883, \n",
      "Epoch 1034 - Reconstruction Loss: 0.0839, \n",
      "Epoch 1035 - Reconstruction Loss: 0.0965, \n",
      "Epoch 1036 - Reconstruction Loss: 0.1073, \n",
      "Epoch 1037 - Reconstruction Loss: 0.0889, \n",
      "Epoch 1038 - Reconstruction Loss: 0.1095, \n",
      "Epoch 1039 - Reconstruction Loss: 0.1034, \n",
      "Epoch 1040 - Reconstruction Loss: 0.0746, \n",
      "Epoch 1041 - Reconstruction Loss: 0.0682, \n",
      "Epoch 1042 - Reconstruction Loss: 0.0953, \n",
      "Epoch 1043 - Reconstruction Loss: 0.0873, \n",
      "Epoch 1044 - Reconstruction Loss: 0.0911, \n",
      "Epoch 1045 - Reconstruction Loss: 0.0807, \n",
      "Epoch 1046 - Reconstruction Loss: 0.0920, \n",
      "Epoch 1047 - Reconstruction Loss: 0.0853, \n",
      "Epoch 1048 - Reconstruction Loss: 0.0954, \n",
      "Epoch 1049 - Reconstruction Loss: 0.1055, \n",
      "Epoch 1050 - Reconstruction Loss: 0.0898, \n",
      "Epoch 1051 - Reconstruction Loss: 0.0884, \n",
      "Epoch 1052 - Reconstruction Loss: 0.1022, \n",
      "Epoch 1053 - Reconstruction Loss: 0.1026, \n",
      "Epoch 1054 - Reconstruction Loss: 0.0977, \n",
      "Epoch 1055 - Reconstruction Loss: 0.0975, \n",
      "Epoch 1056 - Reconstruction Loss: 0.0720, \n",
      "Epoch 1057 - Reconstruction Loss: 0.1008, \n",
      "Epoch 1058 - Reconstruction Loss: 0.0858, \n",
      "Epoch 1059 - Reconstruction Loss: 0.1076, \n",
      "Epoch 1060 - Reconstruction Loss: 0.0984, \n",
      "Epoch 1061 - Reconstruction Loss: 0.0918, \n",
      "Epoch 1062 - Reconstruction Loss: 0.0845, \n",
      "Epoch 1063 - Reconstruction Loss: 0.0995, \n",
      "Epoch 1064 - Reconstruction Loss: 0.1128, \n",
      "Epoch 1065 - Reconstruction Loss: 0.0954, \n",
      "Epoch 1066 - Reconstruction Loss: 0.0831, \n",
      "Epoch 1067 - Reconstruction Loss: 0.1064, \n",
      "Epoch 1068 - Reconstruction Loss: 0.1025, \n",
      "Epoch 1069 - Reconstruction Loss: 0.0995, \n",
      "Epoch 1070 - Reconstruction Loss: 0.1180, \n",
      "Epoch 1071 - Reconstruction Loss: 0.0843, \n",
      "Epoch 1072 - Reconstruction Loss: 0.0895, \n",
      "Epoch 1073 - Reconstruction Loss: 0.0976, \n",
      "Epoch 1074 - Reconstruction Loss: 0.0930, \n",
      "Epoch 1075 - Reconstruction Loss: 0.1134, \n",
      "Epoch 1076 - Reconstruction Loss: 0.1005, \n",
      "Epoch 1077 - Reconstruction Loss: 0.0886, \n",
      "Epoch 1078 - Reconstruction Loss: 0.1016, \n",
      "Epoch 1079 - Reconstruction Loss: 0.0923, \n",
      "Epoch 1080 - Reconstruction Loss: 0.0653, \n",
      "Epoch 1081 - Reconstruction Loss: 0.0831, \n",
      "Epoch 1082 - Reconstruction Loss: 0.1060, \n",
      "Epoch 1083 - Reconstruction Loss: 0.1061, \n",
      "Epoch 1084 - Reconstruction Loss: 0.0891, \n",
      "Epoch 1085 - Reconstruction Loss: 0.1185, \n",
      "Epoch 1086 - Reconstruction Loss: 0.0841, \n",
      "Epoch 1087 - Reconstruction Loss: 0.1051, \n",
      "Epoch 1088 - Reconstruction Loss: 0.0816, \n",
      "Epoch 1089 - Reconstruction Loss: 0.0887, \n",
      "Epoch 1090 - Reconstruction Loss: 0.1202, \n",
      "Epoch 1091 - Reconstruction Loss: 0.0848, \n",
      "Epoch 1092 - Reconstruction Loss: 0.0894, \n",
      "Epoch 1093 - Reconstruction Loss: 0.0972, \n",
      "Epoch 1094 - Reconstruction Loss: 0.1063, \n",
      "Epoch 1095 - Reconstruction Loss: 0.1042, \n",
      "Epoch 1096 - Reconstruction Loss: 0.0804, \n",
      "Epoch 1097 - Reconstruction Loss: 0.0719, \n",
      "Epoch 1098 - Reconstruction Loss: 0.1176, \n",
      "Epoch 1099 - Reconstruction Loss: 0.0902, \n",
      "Epoch 1100 - Reconstruction Loss: 0.0995, \n",
      "Epoch 1101 - Reconstruction Loss: 0.0891, \n",
      "Epoch 1102 - Reconstruction Loss: 0.0993, \n",
      "Epoch 1103 - Reconstruction Loss: 0.1063, \n",
      "Epoch 1104 - Reconstruction Loss: 0.0811, \n",
      "Epoch 1105 - Reconstruction Loss: 0.0848, \n",
      "Epoch 1106 - Reconstruction Loss: 0.0978, \n",
      "Epoch 1107 - Reconstruction Loss: 0.0804, \n",
      "Epoch 1108 - Reconstruction Loss: 0.1182, \n",
      "Epoch 1109 - Reconstruction Loss: 0.1040, \n",
      "Epoch 1110 - Reconstruction Loss: 0.0727, \n",
      "Epoch 1111 - Reconstruction Loss: 0.0948, \n",
      "Epoch 1112 - Reconstruction Loss: 0.0942, \n",
      "Epoch 1113 - Reconstruction Loss: 0.1035, \n",
      "Epoch 1114 - Reconstruction Loss: 0.0948, \n",
      "Epoch 1115 - Reconstruction Loss: 0.0956, \n",
      "Epoch 1116 - Reconstruction Loss: 0.0875, \n",
      "Epoch 1117 - Reconstruction Loss: 0.0795, \n",
      "Epoch 1118 - Reconstruction Loss: 0.0951, \n",
      "Epoch 1119 - Reconstruction Loss: 0.1142, \n",
      "Epoch 1120 - Reconstruction Loss: 0.1045, \n",
      "Epoch 1121 - Reconstruction Loss: 0.0819, \n",
      "Epoch 1122 - Reconstruction Loss: 0.0916, \n",
      "Epoch 1123 - Reconstruction Loss: 0.1079, \n",
      "Epoch 1124 - Reconstruction Loss: 0.0852, \n",
      "Epoch 1125 - Reconstruction Loss: 0.1238, \n",
      "Epoch 1126 - Reconstruction Loss: 0.1025, \n",
      "Epoch 1127 - Reconstruction Loss: 0.0828, \n",
      "Epoch 1128 - Reconstruction Loss: 0.0881, \n",
      "Epoch 1129 - Reconstruction Loss: 0.0963, \n",
      "Epoch 1130 - Reconstruction Loss: 0.0959, \n",
      "Epoch 1131 - Reconstruction Loss: 0.1133, \n",
      "Epoch 1132 - Reconstruction Loss: 0.0850, \n",
      "Epoch 1133 - Reconstruction Loss: 0.0912, \n",
      "Epoch 1134 - Reconstruction Loss: 0.1083, \n",
      "Epoch 1135 - Reconstruction Loss: 0.1010, \n",
      "Epoch 1136 - Reconstruction Loss: 0.0800, \n",
      "Epoch 1137 - Reconstruction Loss: 0.0954, \n",
      "Epoch 1138 - Reconstruction Loss: 0.0846, \n",
      "Epoch 1139 - Reconstruction Loss: 0.1064, \n",
      "Epoch 1140 - Reconstruction Loss: 0.1033, \n",
      "Epoch 1141 - Reconstruction Loss: 0.1315, \n",
      "Epoch 1142 - Reconstruction Loss: 0.0945, \n",
      "Epoch 1143 - Reconstruction Loss: 0.0881, \n",
      "Epoch 1144 - Reconstruction Loss: 0.0989, \n",
      "Epoch 1145 - Reconstruction Loss: 0.0819, \n",
      "Epoch 1146 - Reconstruction Loss: 0.0764, \n",
      "Epoch 1147 - Reconstruction Loss: 0.0842, \n",
      "Epoch 1148 - Reconstruction Loss: 0.1006, \n",
      "Epoch 1149 - Reconstruction Loss: 0.1106, \n",
      "Epoch 1150 - Reconstruction Loss: 0.1041, \n",
      "Epoch 1151 - Reconstruction Loss: 0.0857, \n",
      "Epoch 1152 - Reconstruction Loss: 0.0916, \n",
      "Epoch 1153 - Reconstruction Loss: 0.0869, \n",
      "Epoch 1154 - Reconstruction Loss: 0.0814, \n",
      "Epoch 1155 - Reconstruction Loss: 0.1064, \n",
      "Epoch 1156 - Reconstruction Loss: 0.1129, \n",
      "Epoch 1157 - Reconstruction Loss: 0.1009, \n",
      "Epoch 1158 - Reconstruction Loss: 0.0997, \n",
      "Epoch 1159 - Reconstruction Loss: 0.0812, \n",
      "Epoch 1160 - Reconstruction Loss: 0.0905, \n",
      "Epoch 1161 - Reconstruction Loss: 0.0892, \n",
      "Epoch 1162 - Reconstruction Loss: 0.0807, \n",
      "Epoch 1163 - Reconstruction Loss: 0.0937, \n",
      "Epoch 1164 - Reconstruction Loss: 0.0781, \n",
      "Epoch 1165 - Reconstruction Loss: 0.0866, \n",
      "Epoch 1166 - Reconstruction Loss: 0.0873, \n",
      "Epoch 1167 - Reconstruction Loss: 0.1053, \n",
      "Epoch 1168 - Reconstruction Loss: 0.0923, \n",
      "Epoch 1169 - Reconstruction Loss: 0.1031, \n",
      "Epoch 1170 - Reconstruction Loss: 0.1038, \n",
      "Epoch 1171 - Reconstruction Loss: 0.0817, \n",
      "Epoch 1172 - Reconstruction Loss: 0.0841, \n",
      "Epoch 1173 - Reconstruction Loss: 0.1006, \n",
      "Epoch 1174 - Reconstruction Loss: 0.0935, \n",
      "Epoch 1175 - Reconstruction Loss: 0.0954, \n",
      "Epoch 1176 - Reconstruction Loss: 0.0807, \n",
      "Epoch 1177 - Reconstruction Loss: 0.0822, \n",
      "Epoch 1178 - Reconstruction Loss: 0.0889, \n",
      "Epoch 1179 - Reconstruction Loss: 0.0716, \n",
      "Epoch 1180 - Reconstruction Loss: 0.1015, \n",
      "Epoch 1181 - Reconstruction Loss: 0.0885, \n",
      "Epoch 1182 - Reconstruction Loss: 0.0808, \n",
      "Epoch 1183 - Reconstruction Loss: 0.0884, \n",
      "Epoch 1184 - Reconstruction Loss: 0.0920, \n",
      "Epoch 1185 - Reconstruction Loss: 0.1223, \n",
      "Epoch 1186 - Reconstruction Loss: 0.0828, \n",
      "Epoch 1187 - Reconstruction Loss: 0.0943, \n",
      "Epoch 1188 - Reconstruction Loss: 0.1028, \n",
      "Epoch 1189 - Reconstruction Loss: 0.1019, \n",
      "Epoch 1190 - Reconstruction Loss: 0.0846, \n",
      "Epoch 1191 - Reconstruction Loss: 0.0896, \n",
      "Epoch 1192 - Reconstruction Loss: 0.0866, \n",
      "Epoch 1193 - Reconstruction Loss: 0.0840, \n",
      "Epoch 1194 - Reconstruction Loss: 0.0824, \n",
      "Epoch 1195 - Reconstruction Loss: 0.0950, \n",
      "Epoch 1196 - Reconstruction Loss: 0.1028, \n",
      "Epoch 1197 - Reconstruction Loss: 0.0840, \n",
      "Epoch 1198 - Reconstruction Loss: 0.0979, \n",
      "Epoch 1199 - Reconstruction Loss: 0.0868, \n",
      "Epoch 1200 - Reconstruction Loss: 0.0897, \n",
      "Epoch 1201 - Reconstruction Loss: 0.0838, \n",
      "Epoch 1202 - Reconstruction Loss: 0.1059, \n",
      "Epoch 1203 - Reconstruction Loss: 0.0957, \n",
      "Epoch 1204 - Reconstruction Loss: 0.0998, \n",
      "Epoch 1205 - Reconstruction Loss: 0.0971, \n",
      "Epoch 1206 - Reconstruction Loss: 0.0870, \n",
      "Epoch 1207 - Reconstruction Loss: 0.0879, \n",
      "Epoch 1208 - Reconstruction Loss: 0.0958, \n",
      "Epoch 1209 - Reconstruction Loss: 0.1029, \n",
      "Epoch 1210 - Reconstruction Loss: 0.1084, \n",
      "Epoch 1211 - Reconstruction Loss: 0.0788, \n",
      "Epoch 1212 - Reconstruction Loss: 0.0963, \n",
      "Epoch 1213 - Reconstruction Loss: 0.1012, \n",
      "Epoch 1214 - Reconstruction Loss: 0.1225, \n",
      "Epoch 1215 - Reconstruction Loss: 0.0831, \n",
      "Epoch 1216 - Reconstruction Loss: 0.0793, \n",
      "Epoch 1217 - Reconstruction Loss: 0.1128, \n",
      "Epoch 1218 - Reconstruction Loss: 0.0852, \n",
      "Epoch 1219 - Reconstruction Loss: 0.0943, \n",
      "Epoch 1220 - Reconstruction Loss: 0.0976, \n",
      "Epoch 1221 - Reconstruction Loss: 0.1129, \n",
      "Epoch 1222 - Reconstruction Loss: 0.0807, \n",
      "Epoch 1223 - Reconstruction Loss: 0.1043, \n",
      "Epoch 1224 - Reconstruction Loss: 0.0944, \n",
      "Epoch 1225 - Reconstruction Loss: 0.1155, \n",
      "Epoch 1226 - Reconstruction Loss: 0.0902, \n",
      "Epoch 1227 - Reconstruction Loss: 0.0908, \n",
      "Epoch 1228 - Reconstruction Loss: 0.0831, \n",
      "Epoch 1229 - Reconstruction Loss: 0.0767, \n",
      "Epoch 1230 - Reconstruction Loss: 0.0893, \n",
      "Epoch 1231 - Reconstruction Loss: 0.0915, \n",
      "Epoch 1232 - Reconstruction Loss: 0.0760, \n",
      "Epoch 1233 - Reconstruction Loss: 0.1055, \n",
      "Epoch 1234 - Reconstruction Loss: 0.0854, \n",
      "Epoch 1235 - Reconstruction Loss: 0.0949, \n",
      "Epoch 1236 - Reconstruction Loss: 0.0799, \n",
      "Epoch 1237 - Reconstruction Loss: 0.0822, \n",
      "Epoch 1238 - Reconstruction Loss: 0.0800, \n",
      "Epoch 1239 - Reconstruction Loss: 0.0881, \n",
      "Epoch 1240 - Reconstruction Loss: 0.1151, \n",
      "Epoch 1241 - Reconstruction Loss: 0.0978, \n",
      "Epoch 1242 - Reconstruction Loss: 0.0904, \n",
      "Epoch 1243 - Reconstruction Loss: 0.0854, \n",
      "Epoch 1244 - Reconstruction Loss: 0.0851, \n",
      "Epoch 1245 - Reconstruction Loss: 0.0961, \n",
      "Epoch 1246 - Reconstruction Loss: 0.0856, \n",
      "Epoch 1247 - Reconstruction Loss: 0.1082, \n",
      "Epoch 1248 - Reconstruction Loss: 0.0950, \n",
      "Epoch 1249 - Reconstruction Loss: 0.0871, \n",
      "Epoch 1250 - Reconstruction Loss: 0.0917, \n",
      "Epoch 1251 - Reconstruction Loss: 0.0955, \n",
      "Epoch 1252 - Reconstruction Loss: 0.0803, \n",
      "Epoch 1253 - Reconstruction Loss: 0.1041, \n",
      "Epoch 1254 - Reconstruction Loss: 0.0993, \n",
      "Epoch 1255 - Reconstruction Loss: 0.0821, \n",
      "Epoch 1256 - Reconstruction Loss: 0.1243, \n",
      "Epoch 1257 - Reconstruction Loss: 0.0937, \n",
      "Epoch 1258 - Reconstruction Loss: 0.0750, \n",
      "Epoch 1259 - Reconstruction Loss: 0.0920, \n",
      "Epoch 1260 - Reconstruction Loss: 0.1046, \n",
      "Epoch 1261 - Reconstruction Loss: 0.0913, \n",
      "Epoch 1262 - Reconstruction Loss: 0.0957, \n",
      "Epoch 1263 - Reconstruction Loss: 0.0858, \n",
      "Epoch 1264 - Reconstruction Loss: 0.0802, \n",
      "Epoch 1265 - Reconstruction Loss: 0.1006, \n",
      "Epoch 1266 - Reconstruction Loss: 0.1096, \n",
      "Epoch 1267 - Reconstruction Loss: 0.0864, \n",
      "Epoch 1268 - Reconstruction Loss: 0.0820, \n",
      "Epoch 1269 - Reconstruction Loss: 0.0914, \n",
      "Epoch 1270 - Reconstruction Loss: 0.0985, \n",
      "Epoch 1271 - Reconstruction Loss: 0.1174, \n",
      "Epoch 1272 - Reconstruction Loss: 0.0730, \n",
      "Epoch 1273 - Reconstruction Loss: 0.0914, \n",
      "Epoch 1274 - Reconstruction Loss: 0.0806, \n",
      "Epoch 1275 - Reconstruction Loss: 0.0833, \n",
      "Epoch 1276 - Reconstruction Loss: 0.0926, \n",
      "Epoch 1277 - Reconstruction Loss: 0.0892, \n",
      "Epoch 1278 - Reconstruction Loss: 0.0954, \n",
      "Epoch 1279 - Reconstruction Loss: 0.0962, \n",
      "Epoch 1280 - Reconstruction Loss: 0.0760, \n",
      "Epoch 1281 - Reconstruction Loss: 0.0869, \n",
      "Epoch 1282 - Reconstruction Loss: 0.0730, \n",
      "Epoch 1283 - Reconstruction Loss: 0.0805, \n",
      "Epoch 1284 - Reconstruction Loss: 0.0929, \n",
      "Epoch 1285 - Reconstruction Loss: 0.0935, \n",
      "Epoch 1286 - Reconstruction Loss: 0.1309, \n",
      "Epoch 1287 - Reconstruction Loss: 0.0980, \n",
      "Epoch 1288 - Reconstruction Loss: 0.1056, \n",
      "Epoch 1289 - Reconstruction Loss: 0.0961, \n",
      "Epoch 1290 - Reconstruction Loss: 0.0991, \n",
      "Epoch 1291 - Reconstruction Loss: 0.0988, \n",
      "Epoch 1292 - Reconstruction Loss: 0.1126, \n",
      "Epoch 1293 - Reconstruction Loss: 0.0861, \n",
      "Epoch 1294 - Reconstruction Loss: 0.1042, \n",
      "Epoch 1295 - Reconstruction Loss: 0.0752, \n",
      "Epoch 1296 - Reconstruction Loss: 0.1067, \n",
      "Epoch 1297 - Reconstruction Loss: 0.1057, \n",
      "Epoch 1298 - Reconstruction Loss: 0.1063, \n",
      "Epoch 1299 - Reconstruction Loss: 0.1003, \n",
      "Epoch 1300 - Reconstruction Loss: 0.1369, \n",
      "Epoch 1301 - Reconstruction Loss: 0.0917, \n",
      "Epoch 1302 - Reconstruction Loss: 0.0768, \n",
      "Epoch 1303 - Reconstruction Loss: 0.1102, \n",
      "Epoch 1304 - Reconstruction Loss: 0.0799, \n",
      "Epoch 1305 - Reconstruction Loss: 0.1126, \n",
      "Epoch 1306 - Reconstruction Loss: 0.0914, \n",
      "Epoch 1307 - Reconstruction Loss: 0.0998, \n",
      "Epoch 1308 - Reconstruction Loss: 0.0919, \n",
      "Epoch 1309 - Reconstruction Loss: 0.0732, \n",
      "Epoch 1310 - Reconstruction Loss: 0.1044, \n",
      "Epoch 1311 - Reconstruction Loss: 0.0866, \n",
      "Epoch 1312 - Reconstruction Loss: 0.0928, \n",
      "Epoch 1313 - Reconstruction Loss: 0.0982, \n",
      "Epoch 1314 - Reconstruction Loss: 0.0827, \n",
      "Epoch 1315 - Reconstruction Loss: 0.0701, \n",
      "Epoch 1316 - Reconstruction Loss: 0.0909, \n",
      "Epoch 1317 - Reconstruction Loss: 0.1020, \n",
      "Epoch 1318 - Reconstruction Loss: 0.1061, \n",
      "Epoch 1319 - Reconstruction Loss: 0.0991, \n",
      "Epoch 1320 - Reconstruction Loss: 0.0950, \n",
      "Epoch 1321 - Reconstruction Loss: 0.0814, \n",
      "Epoch 1322 - Reconstruction Loss: 0.0830, \n",
      "Epoch 1323 - Reconstruction Loss: 0.0881, \n",
      "Epoch 1324 - Reconstruction Loss: 0.1021, \n",
      "Epoch 1325 - Reconstruction Loss: 0.1082, \n",
      "Epoch 1326 - Reconstruction Loss: 0.1091, \n",
      "Epoch 1327 - Reconstruction Loss: 0.0788, \n",
      "Epoch 1328 - Reconstruction Loss: 0.1175, \n",
      "Epoch 1329 - Reconstruction Loss: 0.0908, \n",
      "Epoch 1330 - Reconstruction Loss: 0.0865, \n",
      "Epoch 1331 - Reconstruction Loss: 0.0818, \n",
      "Epoch 1332 - Reconstruction Loss: 0.0978, \n",
      "Epoch 1333 - Reconstruction Loss: 0.0962, \n",
      "Epoch 1334 - Reconstruction Loss: 0.0886, \n",
      "Epoch 1335 - Reconstruction Loss: 0.0886, \n",
      "Epoch 1336 - Reconstruction Loss: 0.0978, \n",
      "Epoch 1337 - Reconstruction Loss: 0.1042, \n",
      "Epoch 1338 - Reconstruction Loss: 0.0745, \n",
      "Epoch 1339 - Reconstruction Loss: 0.1053, \n",
      "Epoch 1340 - Reconstruction Loss: 0.1017, \n",
      "Epoch 1341 - Reconstruction Loss: 0.1133, \n",
      "Epoch 1342 - Reconstruction Loss: 0.0942, \n",
      "Epoch 1343 - Reconstruction Loss: 0.1180, \n",
      "Epoch 1344 - Reconstruction Loss: 0.1071, \n",
      "Epoch 1345 - Reconstruction Loss: 0.1117, \n",
      "Epoch 1346 - Reconstruction Loss: 0.0992, \n",
      "Epoch 1347 - Reconstruction Loss: 0.0965, \n",
      "Epoch 1348 - Reconstruction Loss: 0.1051, \n",
      "Epoch 1349 - Reconstruction Loss: 0.0883, \n",
      "Epoch 1350 - Reconstruction Loss: 0.1024, \n",
      "Epoch 1351 - Reconstruction Loss: 0.0979, \n",
      "Epoch 1352 - Reconstruction Loss: 0.1044, \n",
      "Epoch 1353 - Reconstruction Loss: 0.0898, \n",
      "Epoch 1354 - Reconstruction Loss: 0.0935, \n",
      "Epoch 1355 - Reconstruction Loss: 0.0788, \n",
      "Epoch 1356 - Reconstruction Loss: 0.1122, \n",
      "Epoch 1357 - Reconstruction Loss: 0.1034, \n",
      "Epoch 1358 - Reconstruction Loss: 0.0778, \n",
      "Epoch 1359 - Reconstruction Loss: 0.0965, \n",
      "Epoch 1360 - Reconstruction Loss: 0.1000, \n",
      "Epoch 1361 - Reconstruction Loss: 0.0908, \n",
      "Epoch 1362 - Reconstruction Loss: 0.1001, \n",
      "Epoch 1363 - Reconstruction Loss: 0.0987, \n",
      "Epoch 1364 - Reconstruction Loss: 0.0932, \n",
      "Epoch 1365 - Reconstruction Loss: 0.1055, \n",
      "Epoch 1366 - Reconstruction Loss: 0.1038, \n",
      "Epoch 1367 - Reconstruction Loss: 0.1118, \n",
      "Epoch 1368 - Reconstruction Loss: 0.1054, \n",
      "Epoch 1369 - Reconstruction Loss: 0.0763, \n",
      "Epoch 1370 - Reconstruction Loss: 0.0888, \n",
      "Epoch 1371 - Reconstruction Loss: 0.0931, \n",
      "Epoch 1372 - Reconstruction Loss: 0.0777, \n",
      "Epoch 1373 - Reconstruction Loss: 0.1015, \n",
      "Epoch 1374 - Reconstruction Loss: 0.0950, \n",
      "Epoch 1375 - Reconstruction Loss: 0.1013, \n",
      "Epoch 1376 - Reconstruction Loss: 0.0878, \n",
      "Epoch 1377 - Reconstruction Loss: 0.0891, \n",
      "Epoch 1378 - Reconstruction Loss: 0.0814, \n",
      "Epoch 1379 - Reconstruction Loss: 0.0920, \n",
      "Epoch 1380 - Reconstruction Loss: 0.1007, \n",
      "Epoch 1381 - Reconstruction Loss: 0.0884, \n",
      "Epoch 1382 - Reconstruction Loss: 0.0838, \n",
      "Epoch 1383 - Reconstruction Loss: 0.0962, \n",
      "Epoch 1384 - Reconstruction Loss: 0.1027, \n",
      "Epoch 1385 - Reconstruction Loss: 0.1077, \n",
      "Epoch 1386 - Reconstruction Loss: 0.0796, \n",
      "Epoch 1387 - Reconstruction Loss: 0.0977, \n",
      "Epoch 1388 - Reconstruction Loss: 0.0810, \n",
      "Epoch 1389 - Reconstruction Loss: 0.0958, \n",
      "Epoch 1390 - Reconstruction Loss: 0.0952, \n",
      "Epoch 1391 - Reconstruction Loss: 0.0952, \n",
      "Epoch 1392 - Reconstruction Loss: 0.0877, \n",
      "Epoch 1393 - Reconstruction Loss: 0.0996, \n",
      "Epoch 1394 - Reconstruction Loss: 0.1108, \n",
      "Epoch 1395 - Reconstruction Loss: 0.1045, \n",
      "Epoch 1396 - Reconstruction Loss: 0.0836, \n",
      "Epoch 1397 - Reconstruction Loss: 0.0968, \n",
      "Epoch 1398 - Reconstruction Loss: 0.0706, \n",
      "Epoch 1399 - Reconstruction Loss: 0.0816, \n",
      "Epoch 1400 - Reconstruction Loss: 0.1229, \n",
      "Epoch 1401 - Reconstruction Loss: 0.0793, \n",
      "Epoch 1402 - Reconstruction Loss: 0.0875, \n",
      "Epoch 1403 - Reconstruction Loss: 0.0763, \n",
      "Epoch 1404 - Reconstruction Loss: 0.1015, \n",
      "Epoch 1405 - Reconstruction Loss: 0.1051, \n",
      "Epoch 1406 - Reconstruction Loss: 0.0892, \n",
      "Epoch 1407 - Reconstruction Loss: 0.1296, \n",
      "Epoch 1408 - Reconstruction Loss: 0.0967, \n",
      "Epoch 1409 - Reconstruction Loss: 0.0936, \n",
      "Epoch 1410 - Reconstruction Loss: 0.0911, \n",
      "Epoch 1411 - Reconstruction Loss: 0.1084, \n",
      "Epoch 1412 - Reconstruction Loss: 0.0939, \n",
      "Epoch 1413 - Reconstruction Loss: 0.1024, \n",
      "Epoch 1414 - Reconstruction Loss: 0.0871, \n",
      "Epoch 1415 - Reconstruction Loss: 0.1025, \n",
      "Epoch 1416 - Reconstruction Loss: 0.1247, \n",
      "Epoch 1417 - Reconstruction Loss: 0.0903, \n",
      "Epoch 1418 - Reconstruction Loss: 0.0936, \n",
      "Epoch 1419 - Reconstruction Loss: 0.1085, \n",
      "Epoch 1420 - Reconstruction Loss: 0.1061, \n",
      "Epoch 1421 - Reconstruction Loss: 0.0915, \n",
      "Epoch 1422 - Reconstruction Loss: 0.1028, \n",
      "Epoch 1423 - Reconstruction Loss: 0.0932, \n",
      "Epoch 1424 - Reconstruction Loss: 0.0898, \n",
      "Epoch 1425 - Reconstruction Loss: 0.0882, \n",
      "Epoch 1426 - Reconstruction Loss: 0.0913, \n",
      "Epoch 1427 - Reconstruction Loss: 0.0924, \n",
      "Epoch 1428 - Reconstruction Loss: 0.1201, \n",
      "Epoch 1429 - Reconstruction Loss: 0.0969, \n",
      "Epoch 1430 - Reconstruction Loss: 0.0782, \n",
      "Epoch 1431 - Reconstruction Loss: 0.0924, \n",
      "Epoch 1432 - Reconstruction Loss: 0.1073, \n",
      "Epoch 1433 - Reconstruction Loss: 0.0894, \n",
      "Epoch 1434 - Reconstruction Loss: 0.0978, \n",
      "Epoch 1435 - Reconstruction Loss: 0.0896, \n",
      "Epoch 1436 - Reconstruction Loss: 0.1098, \n",
      "Epoch 1437 - Reconstruction Loss: 0.1074, \n",
      "Epoch 1438 - Reconstruction Loss: 0.0890, \n",
      "Epoch 1439 - Reconstruction Loss: 0.0908, \n",
      "Epoch 1440 - Reconstruction Loss: 0.0986, \n",
      "Epoch 1441 - Reconstruction Loss: 0.0788, \n",
      "Epoch 1442 - Reconstruction Loss: 0.0888, \n",
      "Epoch 1443 - Reconstruction Loss: 0.1079, \n",
      "Epoch 1444 - Reconstruction Loss: 0.0881, \n",
      "Epoch 1445 - Reconstruction Loss: 0.0814, \n",
      "Epoch 1446 - Reconstruction Loss: 0.0819, \n",
      "Epoch 1447 - Reconstruction Loss: 0.0948, \n",
      "Epoch 1448 - Reconstruction Loss: 0.0823, \n",
      "Epoch 1449 - Reconstruction Loss: 0.0823, \n",
      "Epoch 1450 - Reconstruction Loss: 0.0962, \n",
      "Epoch 1451 - Reconstruction Loss: 0.0843, \n",
      "Epoch 1452 - Reconstruction Loss: 0.0956, \n",
      "Epoch 1453 - Reconstruction Loss: 0.0876, \n",
      "Epoch 1454 - Reconstruction Loss: 0.0943, \n",
      "Epoch 1455 - Reconstruction Loss: 0.0794, \n",
      "Epoch 1456 - Reconstruction Loss: 0.0953, \n",
      "Epoch 1457 - Reconstruction Loss: 0.1016, \n",
      "Epoch 1458 - Reconstruction Loss: 0.1160, \n",
      "Epoch 1459 - Reconstruction Loss: 0.0951, \n",
      "Epoch 1460 - Reconstruction Loss: 0.0956, \n",
      "Epoch 1461 - Reconstruction Loss: 0.0892, \n",
      "Epoch 1462 - Reconstruction Loss: 0.0985, \n",
      "Epoch 1463 - Reconstruction Loss: 0.0972, \n",
      "Epoch 1464 - Reconstruction Loss: 0.1087, \n",
      "Epoch 1465 - Reconstruction Loss: 0.0870, \n",
      "Epoch 1466 - Reconstruction Loss: 0.0786, \n",
      "Epoch 1467 - Reconstruction Loss: 0.1048, \n",
      "Epoch 1468 - Reconstruction Loss: 0.0865, \n",
      "Epoch 1469 - Reconstruction Loss: 0.1095, \n",
      "Epoch 1470 - Reconstruction Loss: 0.0932, \n",
      "Epoch 1471 - Reconstruction Loss: 0.0996, \n",
      "Epoch 1472 - Reconstruction Loss: 0.0769, \n",
      "Epoch 1473 - Reconstruction Loss: 0.0933, \n",
      "Epoch 1474 - Reconstruction Loss: 0.0978, \n",
      "Epoch 1475 - Reconstruction Loss: 0.0898, \n",
      "Epoch 1476 - Reconstruction Loss: 0.0904, \n",
      "Epoch 1477 - Reconstruction Loss: 0.0723, \n",
      "Epoch 1478 - Reconstruction Loss: 0.0812, \n",
      "Epoch 1479 - Reconstruction Loss: 0.1108, \n",
      "Epoch 1480 - Reconstruction Loss: 0.0908, \n",
      "Epoch 1481 - Reconstruction Loss: 0.0903, \n",
      "Epoch 1482 - Reconstruction Loss: 0.1090, \n",
      "Epoch 1483 - Reconstruction Loss: 0.1040, \n",
      "Epoch 1484 - Reconstruction Loss: 0.0780, \n",
      "Epoch 1485 - Reconstruction Loss: 0.0958, \n",
      "Epoch 1486 - Reconstruction Loss: 0.1090, \n",
      "Epoch 1487 - Reconstruction Loss: 0.0896, \n",
      "Epoch 1488 - Reconstruction Loss: 0.1013, \n",
      "Epoch 1489 - Reconstruction Loss: 0.0994, \n",
      "Epoch 1490 - Reconstruction Loss: 0.0874, \n",
      "Epoch 1491 - Reconstruction Loss: 0.0917, \n",
      "Epoch 1492 - Reconstruction Loss: 0.1005, \n",
      "Epoch 1493 - Reconstruction Loss: 0.1146, \n",
      "Epoch 1494 - Reconstruction Loss: 0.0917, \n",
      "Epoch 1495 - Reconstruction Loss: 0.0943, \n",
      "Epoch 1496 - Reconstruction Loss: 0.1051, \n",
      "Epoch 1497 - Reconstruction Loss: 0.0825, \n",
      "Epoch 1498 - Reconstruction Loss: 0.0983, \n",
      "Epoch 1499 - Reconstruction Loss: 0.0964, \n",
      "Epoch 1500 - Reconstruction Loss: 0.0975, \n",
      "Epoch 1501 - Reconstruction Loss: 0.0947, \n",
      "Epoch 1502 - Reconstruction Loss: 0.1009, \n",
      "Epoch 1503 - Reconstruction Loss: 0.0880, \n",
      "Epoch 1504 - Reconstruction Loss: 0.1064, \n",
      "Epoch 1505 - Reconstruction Loss: 0.1165, \n",
      "Epoch 1506 - Reconstruction Loss: 0.0816, \n",
      "Epoch 1507 - Reconstruction Loss: 0.0785, \n",
      "Epoch 1508 - Reconstruction Loss: 0.0857, \n",
      "Epoch 1509 - Reconstruction Loss: 0.0946, \n",
      "Epoch 1510 - Reconstruction Loss: 0.1149, \n",
      "Epoch 1511 - Reconstruction Loss: 0.0852, \n",
      "Epoch 1512 - Reconstruction Loss: 0.0814, \n",
      "Epoch 1513 - Reconstruction Loss: 0.0822, \n",
      "Epoch 1514 - Reconstruction Loss: 0.0954, \n",
      "Epoch 1515 - Reconstruction Loss: 0.0926, \n",
      "Epoch 1516 - Reconstruction Loss: 0.1058, \n",
      "Epoch 1517 - Reconstruction Loss: 0.1057, \n",
      "Epoch 1518 - Reconstruction Loss: 0.0932, \n",
      "Epoch 1519 - Reconstruction Loss: 0.1103, \n",
      "Epoch 1520 - Reconstruction Loss: 0.0922, \n",
      "Epoch 1521 - Reconstruction Loss: 0.0961, \n",
      "Epoch 1522 - Reconstruction Loss: 0.0884, \n",
      "Epoch 1523 - Reconstruction Loss: 0.1143, \n",
      "Epoch 1524 - Reconstruction Loss: 0.0974, \n",
      "Epoch 1525 - Reconstruction Loss: 0.1063, \n",
      "Epoch 1526 - Reconstruction Loss: 0.0964, \n",
      "Epoch 1527 - Reconstruction Loss: 0.1112, \n",
      "Epoch 1528 - Reconstruction Loss: 0.0899, \n",
      "Epoch 1529 - Reconstruction Loss: 0.1032, \n",
      "Epoch 1530 - Reconstruction Loss: 0.1004, \n",
      "Epoch 1531 - Reconstruction Loss: 0.1059, \n",
      "Epoch 1532 - Reconstruction Loss: 0.0958, \n",
      "Epoch 1533 - Reconstruction Loss: 0.0997, \n",
      "Epoch 1534 - Reconstruction Loss: 0.0983, \n",
      "Epoch 1535 - Reconstruction Loss: 0.0933, \n",
      "Epoch 1536 - Reconstruction Loss: 0.0861, \n",
      "Epoch 1537 - Reconstruction Loss: 0.0839, \n",
      "Epoch 1538 - Reconstruction Loss: 0.1065, \n",
      "Epoch 1539 - Reconstruction Loss: 0.0967, \n",
      "Epoch 1540 - Reconstruction Loss: 0.0958, \n",
      "Epoch 1541 - Reconstruction Loss: 0.1013, \n",
      "Epoch 1542 - Reconstruction Loss: 0.0793, \n",
      "Epoch 1543 - Reconstruction Loss: 0.1171, \n",
      "Epoch 1544 - Reconstruction Loss: 0.1056, \n",
      "Epoch 1545 - Reconstruction Loss: 0.0841, \n",
      "Epoch 1546 - Reconstruction Loss: 0.0806, \n",
      "Epoch 1547 - Reconstruction Loss: 0.1024, \n",
      "Epoch 1548 - Reconstruction Loss: 0.0829, \n",
      "Epoch 1549 - Reconstruction Loss: 0.0961, \n",
      "Epoch 1550 - Reconstruction Loss: 0.1059, \n",
      "Epoch 1551 - Reconstruction Loss: 0.1016, \n",
      "Epoch 1552 - Reconstruction Loss: 0.0942, \n",
      "Epoch 1553 - Reconstruction Loss: 0.0940, \n",
      "Epoch 1554 - Reconstruction Loss: 0.0871, \n",
      "Epoch 1555 - Reconstruction Loss: 0.0875, \n",
      "Epoch 1556 - Reconstruction Loss: 0.0926, \n",
      "Epoch 1557 - Reconstruction Loss: 0.0950, \n",
      "Epoch 1558 - Reconstruction Loss: 0.1007, \n",
      "Epoch 1559 - Reconstruction Loss: 0.1067, \n",
      "Epoch 1560 - Reconstruction Loss: 0.1092, \n",
      "Epoch 1561 - Reconstruction Loss: 0.1055, \n",
      "Epoch 1562 - Reconstruction Loss: 0.0876, \n",
      "Epoch 1563 - Reconstruction Loss: 0.0905, \n",
      "Epoch 1564 - Reconstruction Loss: 0.1052, \n",
      "Epoch 1565 - Reconstruction Loss: 0.1149, \n",
      "Epoch 1566 - Reconstruction Loss: 0.0778, \n",
      "Epoch 1567 - Reconstruction Loss: 0.0803, \n",
      "Epoch 1568 - Reconstruction Loss: 0.0781, \n",
      "Epoch 1569 - Reconstruction Loss: 0.0826, \n",
      "Epoch 1570 - Reconstruction Loss: 0.1120, \n",
      "Epoch 1571 - Reconstruction Loss: 0.0930, \n",
      "Epoch 1572 - Reconstruction Loss: 0.1125, \n",
      "Epoch 1573 - Reconstruction Loss: 0.1005, \n",
      "Epoch 1574 - Reconstruction Loss: 0.0912, \n",
      "Epoch 1575 - Reconstruction Loss: 0.0954, \n",
      "Epoch 1576 - Reconstruction Loss: 0.0917, \n",
      "Epoch 1577 - Reconstruction Loss: 0.0945, \n",
      "Epoch 1578 - Reconstruction Loss: 0.0868, \n",
      "Epoch 1579 - Reconstruction Loss: 0.0904, \n",
      "Epoch 1580 - Reconstruction Loss: 0.1066, \n",
      "Epoch 1581 - Reconstruction Loss: 0.0876, \n",
      "Epoch 1582 - Reconstruction Loss: 0.1034, \n",
      "Epoch 1583 - Reconstruction Loss: 0.0920, \n",
      "Epoch 1584 - Reconstruction Loss: 0.0978, \n",
      "Epoch 1585 - Reconstruction Loss: 0.0784, \n",
      "Epoch 1586 - Reconstruction Loss: 0.0785, \n",
      "Epoch 1587 - Reconstruction Loss: 0.0982, \n",
      "Epoch 1588 - Reconstruction Loss: 0.0911, \n",
      "Epoch 1589 - Reconstruction Loss: 0.0882, \n",
      "Epoch 1590 - Reconstruction Loss: 0.1039, \n",
      "Epoch 1591 - Reconstruction Loss: 0.0988, \n",
      "Epoch 1592 - Reconstruction Loss: 0.1011, \n",
      "Epoch 1593 - Reconstruction Loss: 0.1173, \n",
      "Epoch 1594 - Reconstruction Loss: 0.0939, \n",
      "Epoch 1595 - Reconstruction Loss: 0.1034, \n",
      "Epoch 1596 - Reconstruction Loss: 0.0894, \n",
      "Epoch 1597 - Reconstruction Loss: 0.0783, \n",
      "Epoch 1598 - Reconstruction Loss: 0.0838, \n",
      "Epoch 1599 - Reconstruction Loss: 0.0951, \n"
     ]
    }
   ],
   "source": [
    "trainer.train_data(epochs=10, data_variance=data_variance, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from gridformer\\models\\encoder\n",
      "Model loaded successfully from gridformer\\models\\decoder\n",
      "Model loaded successfully from gridformer\\models\\vq\n"
     ]
    }
   ],
   "source": [
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_t = torch.tensor(np.array(obs[100], dtype=np.float32), dtype=torch.float32, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x, vq_loss, encodings = model(obs_t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ernest\\AppData\\Local\\Temp\\ipykernel_11232\\2610654153.py:1: UserWarning: Using a target size (torch.Size([467])) that is different to the input size (torch.Size([1, 467])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  F.mse_loss(pred_x, obs_t) / data_variance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1320, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(pred_x, obs_t) / data_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_q = model.vq.embedding(latent_indices.to(model.device))  # Maps indices back to codebook vectors\n",
    "\n",
    "# Step 3: Pass z_q to the decoder\n",
    "generated_data = model.decoder(z_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 67,  84, 103,  81,   4,  11,  31,  78,   5, 103, 118,  21,  91,  17,\n",
       "         68, 126,  93,  75,  57,  55,  68, 122,  36, 110,  91, 100,  37,  22,\n",
       "        108,  96,  67,  32,  96,  50,  68,  43,  19,  24,  59,   8,  82, 117,\n",
       "         28,  21,  62,  71,  76, 109,  59,  11,  24,   9,  53,  17,  59,  46,\n",
       "         71,  83, 112,  16,  16, 102,  22,  62])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        print(flat_input.shape)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        print(encoding_indices.shape)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight)\n",
    "        print(quantized.shape)\n",
    "        quantized = quantized.view(input_shape)\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs) # commitment loss\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach()) # detach stop gradient\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach() # trick, \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 64])\n",
      "torch.Size([384, 1])\n",
      "torch.Size([384, 64])\n",
      "Quantization Loss: 1.2756234407424927\n",
      "Quantized Output Shape: torch.Size([8, 3, 32, 32])\n",
      "Perplexity: 245.60606384277344\n",
      "Encodings Shape: torch.Size([384, 512])\n"
     ]
    }
   ],
   "source": [
    "vq = VectorQuantizer(num_embeddings=512, embedding_dim=64, commitment_cost=0.25)\n",
    "\n",
    "# Generate dummy image data (B, C, H, W) -> Batch, Channels, Height, Width\n",
    "dummy_data = torch.randn(8, 3, 32, 32)  # Example: Batch of 8, 64 channels, 32x32 resolution\n",
    "\n",
    "# Forward pass through the VectorQuantizer\n",
    "loss, quantized, perplexity, encodings = vq(dummy_data)\n",
    "\n",
    "# Print results\n",
    "print(\"Quantization Loss:\", loss.item())\n",
    "print(\"Quantized Output Shape:\", quantized.shape)  # Should match the input shape\n",
    "print(\"Perplexity:\", perplexity.item())\n",
    "print(\"Encodings Shape:\", encodings.shape)  # Should be (Ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 64])\n",
      "torch.Size([640, 1])\n",
      "Quantization Loss: 1.2371002435684204\n",
      "Quantized Output Shape: torch.Size([10, 64])\n",
      "Perplexity: 2.0640199184417725\n",
      "Encodings Shape: torch.Size([640, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Modified VectorQuantizer for 1D data\n",
    "class VectorQuantizer1D(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer1D, self).__init__()\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1 / self._num_embeddings, 1 / self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Add a channel dimension to inputs: (B, C, L) -> (B, L, C)\n",
    "        inputs = inputs.unsqueeze(-1)  # Shape (Batch, 467) -> (Batch, 467, 1)\n",
    "        inputs = inputs.permute(0, 2, 1).contiguous()  # Shape (Batch, 467, 1) -> (Batch, 1, 467)\n",
    "        input_shape = inputs.shape  # Save the shape for reshaping later\n",
    "        print(input_shape)\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        print(flat_input.shape)\n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "        \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = torch.nn.functional.mse_loss(quantized.detach(), inputs)  # Commitment loss\n",
    "        q_latent_loss = torch.nn.functional.mse_loss(quantized, inputs.detach())  # Detach gradient\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Trick for gradient flow\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # Reshape quantized output back to (Batch, 467)\n",
    "        quantized = quantized.permute(0, 2, 1).squeeze(-1)\n",
    "        return loss, quantized, perplexity, encodings\n",
    "\n",
    "# Example 1D data\n",
    "num_embeddings = 128\n",
    "embedding_dim = 1\n",
    "commitment_cost = 0.25\n",
    "vq1d = VectorQuantizer1D(num_embeddings=num_embeddings, embedding_dim=embedding_dim, commitment_cost=commitment_cost)\n",
    "\n",
    "dummy_1d_data = torch.randn(10, 64)  # Batch size 10, Data length 467\n",
    "loss, quantized, perplexity, encodings = vq1d(dummy_1d_data)\n",
    "\n",
    "# Print results\n",
    "\n",
    "print(\"Quantization Loss:\", loss.item())\n",
    "print(\"Quantized Output Shape:\", quantized.shape)  # Should match input shape\n",
    "print(\"Perplexity:\", perplexity.item())\n",
    "print(\"Encodings Shape:\", encodings.shape)  # Should be (Batch * 467, num_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output (latent space): tensor([[-0.3739,  0.9837,  0.4223,  1.6036, -0.7556,  0.8580,  0.0707,  0.5382,\n",
      "          1.6785, -0.1444, -0.1950, -0.1330, -0.2951,  0.0589, -1.8625, -1.9954,\n",
      "          1.0577,  0.7093, -0.3857, -0.9472, -0.9413, -0.5421, -2.2130,  0.0392,\n",
      "         -0.5691,  1.6046,  0.5420,  1.0865, -1.3775, -0.1582, -1.3851, -0.7562,\n",
      "         -0.0241,  0.8344, -2.2034,  0.0046,  0.6399,  0.8680, -1.2576, -0.8168,\n",
      "          0.7748,  0.0445, -0.6579,  1.5881,  0.4967, -0.6689, -0.8986,  0.4288,\n",
      "         -0.5296, -0.7484, -0.7317,  0.8956,  2.4192, -0.7451, -0.3974, -0.4048,\n",
      "         -1.6695, -0.0093, -1.9778,  0.4404, -3.1932,  0.5183, -0.7743,  0.0718]])\n",
      "Chunks: tensor([[[-0.3739,  0.9837],\n",
      "         [ 0.4223,  1.6036],\n",
      "         [-0.7556,  0.8580],\n",
      "         [ 0.0707,  0.5382],\n",
      "         [ 1.6785, -0.1444],\n",
      "         [-0.1950, -0.1330],\n",
      "         [-0.2951,  0.0589],\n",
      "         [-1.8625, -1.9954],\n",
      "         [ 1.0577,  0.7093],\n",
      "         [-0.3857, -0.9472],\n",
      "         [-0.9413, -0.5421],\n",
      "         [-2.2130,  0.0392],\n",
      "         [-0.5691,  1.6046],\n",
      "         [ 0.5420,  1.0865],\n",
      "         [-1.3775, -0.1582],\n",
      "         [-1.3851, -0.7562],\n",
      "         [-0.0241,  0.8344],\n",
      "         [-2.2034,  0.0046],\n",
      "         [ 0.6399,  0.8680],\n",
      "         [-1.2576, -0.8168],\n",
      "         [ 0.7748,  0.0445],\n",
      "         [-0.6579,  1.5881],\n",
      "         [ 0.4967, -0.6689],\n",
      "         [-0.8986,  0.4288],\n",
      "         [-0.5296, -0.7484],\n",
      "         [-0.7317,  0.8956],\n",
      "         [ 2.4192, -0.7451],\n",
      "         [-0.3974, -0.4048],\n",
      "         [-1.6695, -0.0093],\n",
      "         [-1.9778,  0.4404],\n",
      "         [-3.1932,  0.5183],\n",
      "         [-0.7743,  0.0718]]])\n",
      "Projected Logits: tensor([[[ 0.7620, -0.7157,  0.8115, -0.4210,  0.3892, -0.2167,  1.0448,\n",
      "           0.4029, -0.2135, -1.5279],\n",
      "         [ 1.6515, -0.7662,  0.5899,  0.1377,  1.0137, -0.2910,  0.9719,\n",
      "           0.2775,  0.0768, -1.5104],\n",
      "         [ 0.4495, -0.7046,  0.9812, -0.6779,  0.1456, -0.2609,  1.1561,\n",
      "           0.4590, -0.2818, -1.6557],\n",
      "         [ 0.7326, -0.6834,  0.3951, -0.1599,  0.4804,  0.1109,  0.6519,\n",
      "           0.3519, -0.3790, -0.9663],\n",
      "         [ 1.2430, -0.6376, -0.7675,  0.8441,  1.1119,  0.8626, -0.3561,\n",
      "           0.1452, -0.5932,  0.4173],\n",
      "         [ 0.1271, -0.6311,  0.2975, -0.3762,  0.1209,  0.3522,  0.4697,\n",
      "           0.4050, -0.6681, -0.6485],\n",
      "         [ 0.1946, -0.6454,  0.4251, -0.4291,  0.1301,  0.2358,  0.5989,\n",
      "           0.4142, -0.5929, -0.8387],\n",
      "         [-2.1107, -0.4824,  0.5533, -1.5826, -1.3705,  0.7709,  0.3716,\n",
      "           0.6812, -1.5096, -0.2919],\n",
      "         [ 1.4382, -0.7004, -0.1005,  0.4944,  1.0601,  0.2972,  0.2956,\n",
      "           0.2108, -0.2663, -0.5282],\n",
      "         [-0.5284, -0.5682,  0.1045, -0.5527, -0.2453,  0.6804,  0.1910,\n",
      "           0.4510, -1.0131, -0.1886],\n",
      "         [-0.5923, -0.5970,  0.5688, -0.8888, -0.4085,  0.3417,  0.6146,\n",
      "           0.5184, -0.8690, -0.7847],\n",
      "         [-0.9686, -0.6364,  1.5037, -1.6803, -0.8946, -0.2721,  1.4303,\n",
      "           0.6809, -0.6825, -1.9079],\n",
      "         [ 1.0577, -0.7624,  1.1515, -0.5084,  0.4876, -0.5587,  1.4066,\n",
      "           0.4151,  0.0351, -2.0708],\n",
      "         [ 1.3796, -0.7271,  0.3309,  0.1825,  0.9090, -0.0176,  0.6893,\n",
      "           0.2733, -0.1321, -1.0823],\n",
      "         [-0.5988, -0.6246,  0.9577, -1.1485, -0.5152,  0.0451,  0.9765,\n",
      "           0.5697, -0.7287, -1.2986],\n",
      "         [-1.0007, -0.5789,  0.7410, -1.1918, -0.7138,  0.3219,  0.7138,\n",
      "           0.5851, -0.9765, -0.8861],\n",
      "         [ 0.8726, -0.7057,  0.5583, -0.2027,  0.5264, -0.0527,  0.8252,\n",
      "           0.3580, -0.2604, -1.2263],\n",
      "         [-0.9859, -0.6338,  1.4855, -1.6763, -0.9007, -0.2534,  1.4107,\n",
      "           0.6804, -0.6964, -1.8784],\n",
      "         [ 1.2931, -0.7108,  0.1947,  0.2323,  0.8899,  0.1106,  0.5492,\n",
      "           0.2650, -0.2183, -0.8748],\n",
      "         [-0.9646, -0.5747,  0.6464, -1.1126, -0.6658,  0.3845,  0.6309,\n",
      "           0.5689, -0.9962, -0.7718],\n",
      "         [ 0.8266, -0.6485, -0.1860,  0.2673,  0.6935,  0.5309,  0.1238,\n",
      "           0.2661, -0.5534, -0.2246],\n",
      "         [ 0.9934, -0.7608,  1.1957, -0.5673,  0.4351, -0.5750,  1.4381,\n",
      "           0.4278,  0.0245, -2.1095],\n",
      "         [ 0.1857, -0.5929, -0.2923,  0.0402,  0.3138,  0.7885, -0.0718,\n",
      "           0.3218, -0.8605,  0.1156],\n",
      "         [ 0.0786, -0.6713,  0.9035, -0.7986, -0.0699, -0.0994,  1.0278,\n",
      "           0.4891, -0.4655, -1.4373],\n",
      "         [-0.4825, -0.5828,  0.2595, -0.6337, -0.2570,  0.5489,  0.3425,\n",
      "           0.4662, -0.9369, -0.4085],\n",
      "         [ 0.4889, -0.7076,  0.9816, -0.6599,  0.1706, -0.2720,  1.1624,\n",
      "           0.4547, -0.2652, -1.6685],\n",
      "         [ 1.2879, -0.5946, -1.4089,  1.2883,  1.3097,  1.3424, -0.9478,\n",
      "           0.0568, -0.8103,  1.2543],\n",
      "         [-0.1749, -0.6096,  0.3116, -0.5255, -0.0751,  0.4244,  0.4374,\n",
      "           0.4396, -0.7891, -0.5733],\n",
      "         [-0.6749, -0.6348,  1.1781, -1.3292, -0.6218, -0.1030,  1.1706,\n",
      "           0.6067, -0.6795, -1.5672],\n",
      "         [-0.5609, -0.6679,  1.5188, -1.5012, -0.6391, -0.3957,  1.5057,\n",
      "           0.6386, -0.5065, -2.0548],\n",
      "         [-1.2380, -0.6691,  2.2358, -2.2884, -1.2591, -0.7597,  2.0728,\n",
      "           0.8055, -0.5259, -2.7954],\n",
      "         [-0.0842, -0.6445,  0.7011, -0.7405, -0.1201,  0.1006,  0.8145,\n",
      "           0.4804, -0.6079, -1.1183]]], grad_fn=<ViewBackward0>)\n",
      "Probabilities: tensor([[[0.1625, 0.0371, 0.1708, 0.0498, 0.1119, 0.0611, 0.2156, 0.1135,\n",
      "          0.0613, 0.0165],\n",
      "         [0.2997, 0.0267, 0.1037, 0.0660, 0.1584, 0.0430, 0.1519, 0.0759,\n",
      "          0.0621, 0.0127],\n",
      "         [0.1218, 0.0384, 0.2073, 0.0394, 0.0899, 0.0599, 0.2469, 0.1230,\n",
      "          0.0586, 0.0148],\n",
      "         [0.1725, 0.0419, 0.1231, 0.0707, 0.1340, 0.0926, 0.1591, 0.1179,\n",
      "          0.0568, 0.0315],\n",
      "         [0.2150, 0.0328, 0.0288, 0.1443, 0.1886, 0.1470, 0.0434, 0.0717,\n",
      "          0.0343, 0.0942],\n",
      "         [0.1093, 0.0512, 0.1296, 0.0661, 0.1087, 0.1369, 0.1540, 0.1444,\n",
      "          0.0494, 0.0503],\n",
      "         [0.1141, 0.0493, 0.1437, 0.0612, 0.1070, 0.1189, 0.1710, 0.1422,\n",
      "          0.0519, 0.0406],\n",
      "         [0.0128, 0.0650, 0.1832, 0.0216, 0.0268, 0.2277, 0.1528, 0.2082,\n",
      "          0.0233, 0.0787],\n",
      "         [0.2732, 0.0322, 0.0586, 0.1063, 0.1872, 0.0873, 0.0872, 0.0801,\n",
      "          0.0497, 0.0382],\n",
      "         [0.0616, 0.0592, 0.1160, 0.0601, 0.0818, 0.2063, 0.1265, 0.1640,\n",
      "          0.0379, 0.0865],\n",
      "         [0.0567, 0.0564, 0.1810, 0.0421, 0.0681, 0.1442, 0.1895, 0.1721,\n",
      "          0.0430, 0.0468],\n",
      "         [0.0280, 0.0390, 0.3314, 0.0137, 0.0301, 0.0561, 0.3079, 0.1456,\n",
      "          0.0372, 0.0109],\n",
      "         [0.1792, 0.0290, 0.1968, 0.0374, 0.1013, 0.0356, 0.2540, 0.0943,\n",
      "          0.0645, 0.0078],\n",
      "         [0.2643, 0.0321, 0.0926, 0.0798, 0.1651, 0.0653, 0.1325, 0.0874,\n",
      "          0.0583, 0.0225],\n",
      "         [0.0507, 0.0494, 0.2406, 0.0293, 0.0552, 0.0966, 0.2452, 0.1632,\n",
      "          0.0446, 0.0252],\n",
      "         [0.0374, 0.0571, 0.2135, 0.0309, 0.0499, 0.1404, 0.2078, 0.1827,\n",
      "          0.0383, 0.0420],\n",
      "         [0.1860, 0.0384, 0.1358, 0.0635, 0.1315, 0.0737, 0.1773, 0.1111,\n",
      "          0.0599, 0.0228],\n",
      "         [0.0278, 0.0396, 0.3293, 0.0139, 0.0303, 0.0579, 0.3055, 0.1472,\n",
      "          0.0371, 0.0114],\n",
      "         [0.2527, 0.0341, 0.0843, 0.0875, 0.1689, 0.0775, 0.1201, 0.0904,\n",
      "          0.0557, 0.0289],\n",
      "         [0.0395, 0.0584, 0.1980, 0.0341, 0.0533, 0.1524, 0.1949, 0.1832,\n",
      "          0.0383, 0.0479],\n",
      "         [0.1835, 0.0420, 0.0667, 0.1049, 0.1606, 0.1365, 0.0909, 0.1047,\n",
      "          0.0462, 0.0641],\n",
      "         [0.1683, 0.0291, 0.2061, 0.0353, 0.0963, 0.0351, 0.2626, 0.0956,\n",
      "          0.0639, 0.0076],\n",
      "         [0.1098, 0.0504, 0.0681, 0.0949, 0.1248, 0.2006, 0.0849, 0.1258,\n",
      "          0.0386, 0.1023],\n",
      "         [0.0929, 0.0439, 0.2120, 0.0387, 0.0801, 0.0778, 0.2401, 0.1401,\n",
      "          0.0539, 0.0204],\n",
      "         [0.0645, 0.0584, 0.1355, 0.0555, 0.0808, 0.1810, 0.1472, 0.1666,\n",
      "          0.0410, 0.0695],\n",
      "         [0.1256, 0.0380, 0.2055, 0.0398, 0.0913, 0.0587, 0.2462, 0.1213,\n",
      "          0.0591, 0.0145],\n",
      "         [0.1728, 0.0263, 0.0117, 0.1729, 0.1766, 0.1825, 0.0185, 0.0505,\n",
      "          0.0212, 0.1671],\n",
      "         [0.0847, 0.0548, 0.1377, 0.0596, 0.0936, 0.1542, 0.1562, 0.1565,\n",
      "          0.0458, 0.0569],\n",
      "         [0.0433, 0.0451, 0.2761, 0.0225, 0.0456, 0.0767, 0.2740, 0.1559,\n",
      "          0.0431, 0.0177],\n",
      "         [0.0402, 0.0361, 0.3215, 0.0157, 0.0371, 0.0474, 0.3173, 0.1333,\n",
      "          0.0424, 0.0090],\n",
      "         [0.0133, 0.0234, 0.4282, 0.0046, 0.0130, 0.0214, 0.3638, 0.1024,\n",
      "          0.0271, 0.0028],\n",
      "         [0.0861, 0.0492, 0.1888, 0.0447, 0.0831, 0.1036, 0.2115, 0.1514,\n",
      "          0.0510, 0.0306]]], grad_fn=<SoftmaxBackward0>)\n",
      "Sampled Indices (stochastic): tensor([[4, 6, 6, 2, 4, 2, 2, 5, 2, 5, 1, 1, 2, 6, 7, 7, 6, 2, 2, 7, 1, 2, 5, 8,\n",
      "         7, 8, 0, 4, 2, 2, 2, 2]])\n",
      "Argmax Indices (deterministic): tensor([[6, 0, 6, 0, 0, 6, 6, 5, 0, 5, 6, 2, 6, 0, 6, 2, 0, 2, 0, 2, 0, 6, 5, 6,\n",
      "         5, 6, 5, 7, 2, 2, 2, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume the encoder output\n",
    "latent_dim = 64  # Output dimension of the encoder\n",
    "num_categories_per_chunk = 10  # Number of categories per chunk\n",
    "num_chunks = 32  # Number of discrete categories\n",
    "\n",
    "# Example latent space output from encoder (batch_size=1 for simplicity)\n",
    "encoder_output = torch.randn(1, latent_dim)  # Shape: (1, 64)\n",
    "\n",
    "# Step 1: Reshape the latent vector into chunks\n",
    "chunk_size = latent_dim // num_chunks  # Size of each chunk (64/32 = 2 here)\n",
    "chunks = encoder_output.view(-1, num_chunks, chunk_size)  # Shape: (batch_size, 32, 2)\n",
    "\n",
    "# Step 2: Map each chunk to a categorical distribution\n",
    "projection_layer = nn.Linear(chunk_size, num_categories_per_chunk)  # Map each chunk to logits\n",
    "projected_logits = projection_layer(chunks)  # Shape: (batch_size, 32, num_categories_per_chunk)\n",
    "\n",
    "# Step 3: Convert logits to probabilities using softmax\n",
    "probabilities = F.softmax(projected_logits, dim=-1)  # Shape: (batch_size, 32, num_categories_per_chunk)\n",
    "\n",
    "# Step 4: Sample discrete indices for each chunk\n",
    "categorical_dist = torch.distributions.Categorical(probabilities)\n",
    "sampled_indices = categorical_dist.sample()  # Shape: (batch_size, 32)\n",
    "\n",
    "# Step 5: (Optional) Use argmax for deterministic indices\n",
    "argmax_indices = torch.argmax(probabilities, dim=-1)  # Shape: (batch_size, 32)\n",
    "\n",
    "# Print results\n",
    "print(\"Encoder Output (latent space):\", encoder_output)\n",
    "print(\"Chunks:\", chunks)\n",
    "print(\"Projected Logits:\", projected_logits)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "print(\"Sampled Indices (stochastic):\", sampled_indices)\n",
    "print(\"Argmax Indices (deterministic):\", argmax_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Latent Space Indices: tensor([ 35,  31,  90,  32, 119,  87,  99, 117,  18, 116])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example cookbook (num_embeddings, embedding_dim)\n",
    "num_embeddings = 128\n",
    "embedding_dim = 64\n",
    "cookbook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "cookbook.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)  # Initialize embeddings\n",
    "\n",
    "# Example encoder output (batch_size, embedding_dim)\n",
    "batch_size = 10\n",
    "encoder_output = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Compute distances between encoder outputs and cookbook embeddings\n",
    "# (batch_size, embedding_dim) -> (batch_size, num_embeddings)\n",
    "distances = (\n",
    "    torch.sum(encoder_output**2, dim=1, keepdim=True)  # (batch_size, 1)\n",
    "    + torch.sum(cookbook.weight**2, dim=1)            # (num_embeddings)\n",
    "    - 2 * torch.matmul(encoder_output, cookbook.weight.t())  # (batch_size, num_embeddings)\n",
    ")\n",
    "\n",
    "# Find the index of the closest embedding\n",
    "indices = torch.argmin(distances, dim=1)  # (batch_size,)\n",
    "\n",
    "print(\"Discrete Latent Space Indices:\", indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = cookbook(indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.9811e-03,  1.6618e-03,  2.1967e-03, -3.0517e-03, -1.6027e-03,\n",
       "        -1.5963e-04, -4.2291e-03, -1.7923e-03,  3.7617e-03, -2.3688e-04,\n",
       "        -6.2850e-03, -7.3592e-03, -2.9147e-03, -6.8549e-03,  2.2048e-03,\n",
       "        -6.6810e-04,  2.4617e-03,  4.1050e-03,  5.8938e-03, -2.2384e-03,\n",
       "        -5.0574e-03, -1.1509e-03,  9.5361e-04,  2.7614e-05,  1.7162e-03,\n",
       "        -5.9978e-03,  4.0929e-03, -6.8525e-03,  7.5781e-03, -5.2806e-03,\n",
       "        -4.5421e-03, -6.2159e-03,  6.3326e-03, -5.6456e-03, -4.6632e-03,\n",
       "         6.1647e-03,  2.2271e-03,  1.3548e-03, -3.9303e-03,  2.4023e-03,\n",
       "         2.2934e-04,  7.6142e-03, -4.8640e-03,  3.9350e-03,  7.3311e-03,\n",
       "        -7.5137e-04,  5.4937e-03, -2.2966e-03,  9.7253e-04,  1.2131e-03,\n",
       "        -2.6793e-03,  3.8246e-03,  7.4413e-03, -6.6107e-03,  2.5234e-03,\n",
       "         7.0236e-03,  3.6490e-03,  2.2222e-03, -7.3964e-03, -5.1530e-03,\n",
       "         5.4228e-03,  4.2103e-03,  6.4655e-03,  2.5434e-03],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Latent Space Shape: torch.Size([10])\n",
      "Decoder Input Shape: torch.Size([10, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example cookbook\n",
    "num_embeddings = 512\n",
    "embedding_dim = 64\n",
    "cookbook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "cookbook.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "# Example encoder output\n",
    "batch_size = 10\n",
    "encoder_output = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Compute distances for each data point to all embeddings\n",
    "# Encoder output is (batch_size, embedding_dim)\n",
    "# Cookbook weights are (num_embeddings, embedding_dim)\n",
    "\n",
    "# Expand dimensions for broadcasting and compute distances\n",
    "encoder_output_flat = encoder_output.unsqueeze(1)  # Shape: (batch_size, 1, embedding_dim)\n",
    "cookbook_weight = cookbook.weight.unsqueeze(0)     # Shape: (1, num_embeddings, embedding_dim)\n",
    "\n",
    "# Calculate squared distances\n",
    "distances = torch.sum((encoder_output_flat - cookbook_weight) ** 2, dim=2)  # Shape: (batch_size, num_embeddings)\n",
    "\n",
    "# Find the index of the closest embedding for each latent vector\n",
    "indices = torch.argmin(distances, dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "# Retrieve discrete latent space from indices\n",
    "discrete_latent_space = indices  # Shape: (batch_size,)\n",
    "decoder_input = cookbook(discrete_latent_space)  # Convert indices to embeddings\n",
    "\n",
    "# Print results\n",
    "print(\"Discrete Latent Space Shape:\", discrete_latent_space.shape)  # Shape: (10,)\n",
    "print(\"Decoder Input Shape:\", decoder_input.shape)  # Shape: (10, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Vector Quantizer for 1D data with latent space compression.\n",
    "\n",
    "        Args:\n",
    "            config (object): Configuration object containing:\n",
    "                - num_embeddings (int): Number of embedding vectors.\n",
    "                - embedding_dim (int): Dimensionality of each embedding vector.\n",
    "                - commitment_cost (float): Weight for commitment loss.\n",
    "        \"\"\"\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.config = config\n",
    "        self._embedding_dim = self.config.embedding_dim\n",
    "        self._num_embeddings = self.config.num_embeddings\n",
    "        self._commitment_cost = self.config.commitment_cost\n",
    "\n",
    "        # Embedding layer\n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1 / self._num_embeddings, 1 / self._num_embeddings)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass of the Vector Quantizer.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): Latent representation from the encoder. Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        Returns:\n",
    "            z_q (torch.Tensor): Quantized representation. Shape: (batch_size, embedding_dim)\n",
    "            loss (torch.Tensor): VQ loss (reconstruction + commitment).\n",
    "            indices (torch.Tensor): Indices of the nearest embeddings. Shape: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Compute distances to each embedding\n",
    "        z_flattened = z.unsqueeze(1)  # Shape: (batch_size, 1, embedding_dim)\n",
    "        embeddings = self._embedding.weight.unsqueeze(0)  # Shape: (1, num_embeddings, embedding_dim)\n",
    "        distances = torch.sum((z_flattened - embeddings) ** 2, dim=2)  # Shape: (batch_size, num_embeddings)\n",
    "\n",
    "        # Find nearest embeddings\n",
    "        indices = torch.argmin(distances, dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Retrieve quantized vectors\n",
    "        z_q = self._embedding(indices)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Compute VQ loss\n",
    "        commitment_loss = self._commitment_cost * F.mse_loss(z.detach(), z_q)\n",
    "        vq_loss = F.mse_loss(z_q, z.detach()) + commitment_loss\n",
    "\n",
    "        # Use straight-through estimator for backpropagation\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        return z_q, vq_loss, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Output Shape: torch.Size([10, 64])\n",
      "VQ Loss: 1.3594753742218018\n",
      "Indices Shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Example configuration\n",
    "class Config:\n",
    "    embedding_dim = 64\n",
    "    num_embeddings = 512\n",
    "    commitment_cost = 0.25\n",
    "\n",
    "# Instantiate the model\n",
    "config = Config()\n",
    "vq_layer = VectorQuantizer(config)\n",
    "\n",
    "# Input latent representation from encoder\n",
    "batch_size = 10\n",
    "latent_dim = config.embedding_dim\n",
    "z = torch.randn(batch_size, latent_dim)\n",
    "\n",
    "# Forward pass\n",
    "z_q, vq_loss, indices = vq_layer(z)\n",
    "\n",
    "# Print results\n",
    "print(\"Quantized Output Shape:\", z_q.shape)  # Should be (batch_size, embedding_dim)\n",
    "print(\"VQ Loss:\", vq_loss.item())\n",
    "print(\"Indices Shape:\", indices.shape)  # Should be (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = vq_layer._embedding(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0721e-04, -1.0879e-03, -1.5866e-03,  1.2390e-03,  4.6564e-04,\n",
       "         1.2915e-03, -1.7815e-03, -1.7502e-03,  1.1934e-03,  1.4162e-03,\n",
       "         1.1394e-03,  1.2946e-05,  1.3202e-03, -4.6986e-04, -1.8610e-03,\n",
       "        -8.4364e-04,  5.0331e-04,  1.4153e-03,  7.8391e-04,  5.7348e-04,\n",
       "         1.5097e-03,  7.4554e-04,  1.3382e-03,  5.1083e-04,  6.0783e-04,\n",
       "        -1.6609e-03, -2.8362e-04, -5.3025e-04, -1.8239e-03,  5.5003e-04,\n",
       "         1.2435e-03,  2.8530e-04,  1.5939e-03, -1.1185e-03, -1.4002e-03,\n",
       "         1.9032e-03,  1.8885e-03,  9.7357e-04, -1.2733e-03, -2.3261e-04,\n",
       "         1.4033e-03,  8.9949e-05, -5.5751e-04, -1.7965e-03, -6.8806e-04,\n",
       "         1.0771e-04, -1.8458e-03,  1.1947e-04,  6.6283e-04,  5.0251e-04,\n",
       "        -1.7891e-03, -1.2311e-03, -6.0251e-04, -1.2800e-03, -6.1387e-04,\n",
       "         1.4836e-03, -1.3507e-03,  8.6210e-04, -6.8157e-04,  9.6605e-04,\n",
       "        -1.4283e-03,  1.7177e-04, -1.5519e-04,  1.0003e-04],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0721e-04, -1.0879e-03, -1.5866e-03,  1.2390e-03,  4.6563e-04,\n",
       "         1.2915e-03, -1.7815e-03, -1.7502e-03,  1.1934e-03,  1.4162e-03,\n",
       "         1.1394e-03,  1.2949e-05,  1.3202e-03, -4.6980e-04, -1.8611e-03,\n",
       "        -8.4364e-04,  5.0330e-04,  1.4153e-03,  7.8391e-04,  5.7352e-04,\n",
       "         1.5096e-03,  7.4553e-04,  1.3382e-03,  5.1093e-04,  6.0783e-04,\n",
       "        -1.6608e-03, -2.8363e-04, -5.3024e-04, -1.8239e-03,  5.5002e-04,\n",
       "         1.2435e-03,  2.8527e-04,  1.5939e-03, -1.1185e-03, -1.4002e-03,\n",
       "         1.9032e-03,  1.8885e-03,  9.7358e-04, -1.2733e-03, -2.3261e-04,\n",
       "         1.4032e-03,  8.9943e-05, -5.5754e-04, -1.7965e-03, -6.8808e-04,\n",
       "         1.0771e-04, -1.8458e-03,  1.1945e-04,  6.6280e-04,  5.0247e-04,\n",
       "        -1.7891e-03, -1.2311e-03, -6.0248e-04, -1.2800e-03, -6.1387e-04,\n",
       "         1.4836e-03, -1.3507e-03,  8.6212e-04, -6.8152e-04,  9.6604e-04,\n",
       "        -1.4283e-03,  1.7178e-04, -1.5521e-04,  1.0002e-04])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2rpn-14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
